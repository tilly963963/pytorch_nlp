{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yelp_train.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM0apB7BsrKDfBdPaosI3lq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tilly963963/pytorch_nlp/blob/main/yelp_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce_9ukeGLvRE",
        "outputId": "a2eba136-770f-4423-dbfa-5919ce648d43"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fftRE3N6MGnO"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/Shareddrives/python教學/PyTorchNLPBook-master/chapters/chapter_3/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UV0h_lp7CRgU"
      },
      "source": [
        "https://blog.csdn.net/weixin_42028364/article/details/81675021"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6Y6ai85WBv5"
      },
      "source": [
        "https://allenlu2007.wordpress.com/2019/02/07/%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E7%9A%84-pytorch-load-dataset/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sduOTBL_MIee"
      },
      "source": [
        "from argparse import Namespace\n",
        "from collections import Counter\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DICLH8lwNyiA"
      },
      "source": [
        "def add_token(token, token_to_idx=None, idx_to_token=None):\n",
        "  if token in token_to_idx:#當查詢用的嗎?\n",
        "    index = token_to_idx[token]\n",
        "    print(\"token in token_to_idx=\",token )\n",
        "  else:\n",
        "    index = len(token_to_idx)#目前長度\n",
        "    token_to_idx[token] = index\n",
        "    idx_to_token[index] = token\n",
        "  # print(\"token(word)=\",token,\"index=\",index)#>25次出現 依照順序 不重複的的\n",
        "  # print(\"token_to_idx=\",token_to_idx)\n",
        "  # print(\"idx_to_token=\",idx_to_token)\n",
        "\n",
        "  return index, token_to_idx, idx_to_token   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eNF3JNUxNUOL"
      },
      "source": [
        "def Vocabulary__init__(add_unk=True, unk_token=\"<UNK>\"):\n",
        "  # if token_to_idx is None:\n",
        "  token_to_idx = {}\n",
        "  # token_to_idx = token_to_idx\n",
        "  # idx_to_token = {idx: token \n",
        "  #             for token, idx in token_to_idx.items()}\n",
        "  # # print(\"idx_to_token=\",idx_to_token) {}\n",
        "  # add_unk = add_unk\n",
        "  # unk_token = unk_token\n",
        "\n",
        "  idx_to_token={}\n",
        "  unk_index = -1\n",
        "\n",
        "  if add_unk:#若有沒見過的字的處理\n",
        "    unk_index, token_to_idx, idx_to_token = add_token(unk_token, token_to_idx, idx_to_token) \n",
        "\n",
        "  print(\"unk_index\",unk_index)\n",
        "  return unk_index, token_to_idx, idx_to_token "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPic04K2MvC5"
      },
      "source": [
        "def from_dataframe(review_df, cutoff=25):\n",
        "  '''\n",
        "  沒見過的字的處理\n",
        "  '''\n",
        "  review_vocab, review_token_to_idx, review_idx_to_token  = Vocabulary__init__(add_unk=True)\n",
        "  print(\"review_vocab, review_token_to_idx, review_idx_to_token=\",review_vocab, review_token_to_idx, review_idx_to_token)\n",
        "  # review_vocab, review_token_to_idx, review_idx_to_token= 0 {'<UNK>': 0} {0: '<UNK>'}\n",
        "  rating_vocab, rating_token_to_idx, rating_idx_to_token  = Vocabulary__init__(add_unk=False)\n",
        "  print(\"rating_vocab, rating_token_to_idx, rating_idx_to_token=\",rating_vocab, rating_token_to_idx, rating_idx_to_token)\n",
        "  # rating_vocab, rating_token_to_idx, rating_idx_to_token= -1 {} {}\n",
        "  '''\n",
        "  真實出現字的處理\n",
        "  '''\n",
        "  # Add ratings\n",
        "  for rating in sorted(set(review_df.rating)):#['negative', 'positive']\n",
        "    rating_vocab, rating_token_to_idx, rating_idx_to_token = add_token(rating, rating_token_to_idx, rating_idx_to_token)\n",
        "  print(\"rating_vocab, rating_token_to_idx, rating_idx_to_token=\",rating_vocab, rating_token_to_idx, rating_idx_to_token)\n",
        "  # rating_vocab, rating_token_to_idx, rating_idx_to_token= 1 {'negative': 0, 'positive': 1} {0: 'negative', 1: 'positive'}\n",
        "  word_counts = Counter()\n",
        "  for review in review_df.review:\n",
        "    for word in review.split(\" \"):\n",
        "      if word not in string.punctuation:\n",
        "        word_counts[word] += 1 #累積出現次數\n",
        "\n",
        "  # print(\"word_counts.items()\",word_counts.items())#每個字出現次數\n",
        "  # word_counts.items() dict_items([('overpriced', 53), \n",
        "  for word, count in word_counts.items():\n",
        "    if count >150:#cutoff:\n",
        "        review_vocab, review_token_to_idx, review_idx_to_token = add_token(word, review_token_to_idx, review_idx_to_token)#依照順序給編號\n",
        "  print(\"review_vocab=\",review_vocab)#常用的字的數量\n",
        "  review_vocab = review_vocab+1\n",
        "  rating_vocab = rating_vocab+1\n",
        "  return review_vocab, review_token_to_idx, review_idx_to_token ,rating_vocab, rating_token_to_idx, rating_idx_to_token "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SE2UCywVMZIV",
        "outputId": "59ae0dc4-b558-4444-fbb6-4f0dad3abf38"
      },
      "source": [
        "review_csv='data/yelp/reviews_with_splits_lite.csv'\n",
        "review_df = pd.read_csv(review_csv)\n",
        "train_review_df = review_df[review_df.split=='train']\n",
        "print(review_df.rating)\n",
        "print(\"--\")\n",
        "print(sorted(set(review_df.rating)))\n",
        "review_vocab, review_token_to_idx, review_idx_to_token ,rating_vocab, rating_token_to_idx, rating_idx_to_token = from_dataframe(train_review_df)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0       negative\n",
            "1       negative\n",
            "2       negative\n",
            "3       negative\n",
            "4       negative\n",
            "          ...   \n",
            "3795    positive\n",
            "3796    positive\n",
            "3797    positive\n",
            "3798    positive\n",
            "3799    positive\n",
            "Name: rating, Length: 3800, dtype: object\n",
            "--\n",
            "['negative', 'positive']\n",
            "unk_index 0\n",
            "review_vocab, review_token_to_idx, review_idx_to_token= 0 {'<UNK>': 0} {0: '<UNK>'}\n",
            "unk_index 0\n",
            "rating_vocab, rating_token_to_idx, rating_idx_to_token= 0 {} {}\n",
            "rating_vocab, rating_token_to_idx, rating_idx_to_token= 1 {'negative': 0, 'positive': 1} {0: 'negative', 1: 'positive'}\n",
            "review_vocab= 306\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7o8wykwGLZ0H"
      },
      "source": [
        "class ReviewClassifier(nn.Module):\n",
        "    \"\"\" a simple perceptron based classifier \"\"\"\n",
        "    def __init__(self, num_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            num_features (int): the size of the input feature vector\n",
        "        \"\"\"\n",
        "        super(ReviewClassifier, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=num_features, \n",
        "                             out_features=1)\n",
        "    def forward(self, x_in, apply_sigmoid=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, num_features)\n",
        "            apply_sigmoid (bool): a flag for the sigmoid activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch,)\n",
        "        \"\"\"\n",
        "        y_out = self.fc1(x_in).squeeze()\n",
        "        if apply_sigmoid:\n",
        "            y_out = torch.sigmoid(y_out)\n",
        "        return y_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZqACPgvLsIQ"
      },
      "source": [
        "class ReviewDataset(Dataset):\n",
        "  def __init__(self, review_df, vectorizer, review_vocab, review_token_to_idx, review_idx_to_token ,rating_vocab, rating_token_to_idx, rating_idx_to_token ):\n",
        "      print(\"===ReviewDataset__init__===\")\n",
        "      self.review_df = review_df\n",
        "      self._vectorizer = vectorizer#307\n",
        "\n",
        "      self.train_df = self.review_df[self.review_df.split=='train']\n",
        "      self.train_size = len(self.train_df)\n",
        "\n",
        "      self.val_df = self.review_df[self.review_df.split=='val']\n",
        "      self.validation_size = len(self.val_df)\n",
        "\n",
        "      self.test_df = self.review_df[self.review_df.split=='test']\n",
        "      self.test_size = len(self.test_df)\n",
        "\n",
        "      self._lookup_dict = {'train': (self.train_df, self.train_size),\n",
        "                            'val': (self.val_df, self.validation_size),\n",
        "                            'test': (self.test_df, self.test_size)}\n",
        "      self.review_vocab, self.review_token_to_idx, self.review_idx_to_token ,self.rating_vocab, self.rating_token_to_idx, self.rating_idx_to_token = review_vocab, review_token_to_idx, review_idx_to_token ,rating_vocab, rating_token_to_idx, rating_idx_to_token\n",
        "      # self.set_split('train')\n",
        "      split=\"train\"\n",
        "      target_split = split#??\n",
        "      self._target_df, self._target_size = self._lookup_dict[split]\n",
        "      print(\"target_df\")\n",
        "      print(self._target_df)\n",
        "      print(\"target_size=\",self._target_size)    \n",
        "  def set_split(self, split=\"train\"):\n",
        "      self._target_split = split\n",
        "      self._target_df, self._target_size = self._lookup_dict[split]\n",
        "      print(\"self._target_df\")\n",
        "      print(self._target_df)\n",
        "      print(\"self._target_size=\",self._target_size)\n",
        "  def review_lookup_token(self, token):\n",
        "      return self.review_token_to_idx.get(token, 0)#返回位置\n",
        "  def rating_lookup_token(self, token):\n",
        "      return self.rating_token_to_idx[token]##返回位置\n",
        "  def rating_lookup_index(self, index):\n",
        "    return self.rating_idx_to_token[index]\n",
        "\n",
        "  def vectorize(self, review, review_vocab):\n",
        "      print(\"review=\",review)\n",
        "      one_hot = np.zeros(review_vocab, dtype=np.float32)\n",
        "      # print(\"one_hot.size\",one_hot.shape)#306\n",
        "      for token in review.split(\" \"):\n",
        "        if token not in string.punctuation:#從字去找位置idx\n",
        "            one_hot[self.review_lookup_token(token)] = 1#在一句話中無法考慮相同的字出現次數 只會在該位置=1只會在該位置=1\n",
        "      # print(\"one_hot.shape=\",one_hot.shape)\n",
        "      return one_hot\n",
        "  def __len__(self):\n",
        "      return self._target_size\n",
        "  def __getitem__(self, index):\n",
        "      \"\"\"the primary entry point method for PyTorch datasets\n",
        "      \n",
        "      Args:\n",
        "          index (int): the index to the data point \n",
        "      Returns:\n",
        "          a dictionary holding the data point's features (x_data) and label (y_target)\n",
        "      \"\"\"\n",
        "      print(\"ReviewDataset __getitem__()\")\n",
        "      row = self._target_df.iloc[index]\n",
        "      # print(\"row=\",row)#一句話\n",
        "      review_vector = self.vectorize(row.review, self._vectorizer)\n",
        "      # print(\"row.review=\",row.review)\n",
        "      # print(\"np.array(review_vector).shape=\",np.array(review_vector).shape)\n",
        "      rating_index = self.rating_lookup_token(row.rating)\n",
        "      # print(\"np.array(rating_index).shape=\",rating_index)\n",
        "      # np.array(review_vector).shape= (3879,)\n",
        "      # np.array(rating_index).shape= ()\n",
        "      print(\"?\")\n",
        "      return {'x_data': review_vector,\n",
        "              'y_target': rating_index}\n",
        "\n",
        "def generate_batches(dataset, batch_size, shuffle=True, drop_last=True, device=\"cpu\"):\n",
        "\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=False, drop_last=drop_last)#Dataset_obj\n",
        "    print(\"generate_batches()\")\n",
        "    # print(\"dataloader\",dataloader) \n",
        "    # x_data= dataloader.__iter__().__next__() #torch.utils.data.dataloader.DataLoader\n",
        "    # print(\"x_data=\",x_data) \n",
        "    for data_dict in dataloader: # x_data = dataloader.__iter__().__next__()\n",
        "        '''\n",
        "        當迭代dataloader時會自動呼叫 \n",
        "        class DataLoader(object)  \n",
        "          def __iter__(self): \n",
        "            return DataLoaderIter(self)\n",
        "        #class DataLoaderIter(object): \n",
        "          def __next__(self):\n",
        "            batch = self.collate_fn([self.dataset[i] for i in indices])\n",
        "            self.dataset[i] => __getitem__() 每一次回傳批次個  用self.collate_fn 打包成批次個\n",
        "        '''\n",
        "        out_data_dict = {}\n",
        "        print(\"data_dict.items()\")\n",
        "        '''\n",
        "        dict_items([('x_data', tensor([[1.,...],\n",
        "        [1.,.]])), ('y_target', tensor([0, 0]))])\n",
        "        '''\n",
        "        print(data_dict.items())\n",
        "        for name, tensor in data_dict.items():#__getitem__\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "        yield out_data_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_9QN9OSSwPx",
        "outputId": "5b8bea5c-ec23-478f-be2a-676cd6d86e03"
      },
      "source": [
        "dataset = ReviewDataset(review_df, review_vocab, review_vocab, review_token_to_idx, review_idx_to_token ,rating_vocab, rating_token_to_idx, rating_idx_to_token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===ReviewDataset__init__===\n",
            "target_df\n",
            "        rating                                             review  split\n",
            "0     negative  overpriced food . they were out of a lot of co...  train\n",
            "1     negative  we had been hearing rave reviews about this pi...  train\n",
            "2     negative  needing a food fix while walking through vario...  train\n",
            "3     negative  walmart makes me cringe . not only for all kin...  train\n",
            "4     negative  we love this place , granted our first time wa...  train\n",
            "...        ...                                                ...    ...\n",
            "3225  positive  wow the pasta is really good at this place . t...  train\n",
            "3226  positive  they re a great local alternative to starbucks...  train\n",
            "3227  positive  i wish there was a way i could give this place...  train\n",
            "3228  positive  went there yesterday and found that the place ...  train\n",
            "3229  positive  all in all , this is one of my fave spots to g...  train\n",
            "\n",
            "[2660 rows x 3 columns]\n",
            "target_size= 2660\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "acgwGwcgLVIx"
      },
      "source": [
        "classifier = ReviewClassifier(num_features=review_vocab)#重複>25次的字有多少個 一句話有3879個數字(0or1 全部不重複大於次數25次的字有3879個)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vweW1e22H0lf"
      },
      "source": [
        "# # Check CUDA\n",
        "# args = Namespace(\n",
        "#     # Data and Path information\n",
        "#     frequency_cutoff=25,\n",
        "#     model_state_file='model.pth',\n",
        "#     review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
        "#     # review_csv='data/yelp/reviews_with_splits_full.csv',\n",
        "#     save_dir='model_storage/ch3/yelp/',\n",
        "#     vectorizer_file='vectorizer.json',\n",
        "#     # No Model hyper parameters\n",
        "#     # Training hyper parameters\n",
        "#     batch_size=2,\n",
        "#     early_stopping_criteria=5,\n",
        "#     learning_rate=0.001,\n",
        "#     num_epochs=100,\n",
        "#     seed=1337,\n",
        "#     # Runtime options\n",
        "#     catch_keyboard_interrupt=True,\n",
        "#     cuda=True,\n",
        "#     expand_filepaths_to_save_dir=True,\n",
        "#     reload_from_files=False,\n",
        "# )\n",
        "# args.cuda = True\n",
        "# if not torch.cuda.is_available():\n",
        "#     args.cuda = False\n",
        "\n",
        "# print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "# args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "# classifier = classifier.to(args.device)\n",
        "\n",
        "# loss_func = nn.BCEWithLogitsLoss()\n",
        "# optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
        "# scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode='min', factor=0.5, patience=1)\n",
        "# for epoch_index in range(1):              \n",
        "#   batch_generator = generate_batches(train_data,batch_size=args.batch_size,device=args.device) \n",
        "#   # batch_generator.__iter__().__next__()\n",
        "#   # print(next(batch_generator))\n",
        "#   print(\"batch_generator=\",enumerate(batch_generator)) \n",
        "#   # for batch_index, batch_dict in enumerate(batch_generator):\n",
        "#   #   print(batch_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41RIEw8XLhAF"
      },
      "source": [
        "# Check CUDA\n",
        "args = Namespace(\n",
        "    # Data and Path information\n",
        "    frequency_cutoff=25,\n",
        "    model_state_file='model.pth',\n",
        "    review_csv='data/yelp/reviews_with_splits_lite.csv',\n",
        "    # review_csv='data/yelp/reviews_with_splits_full.csv',\n",
        "    save_dir='model_storage/ch3/yelp/',\n",
        "    vectorizer_file='vectorizer.json',\n",
        "    # No Model hyper parameters\n",
        "    # Training hyper parameters\n",
        "    batch_size=2,\n",
        "    early_stopping_criteria=5,\n",
        "    learning_rate=0.001,\n",
        "    num_epochs=100,\n",
        "    seed=1337,\n",
        "    # Runtime options\n",
        "    catch_keyboard_interrupt=True,\n",
        "    cuda=True,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        "    reload_from_files=False,\n",
        ")\n",
        "args.cuda = True\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "classifier = classifier.to(args.device)\n",
        "\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,mode='min', factor=0.5, patience=1)\n",
        "for epoch_index in range(3):              \n",
        "  dataset.set_split('train')\n",
        "  batch_generator = generate_batches(dataset,batch_size=args.batch_size,device=args.device) #yiekd => print(next(batch_generator))\n",
        "\n",
        "  running_loss = 0.0\n",
        "  running_acc = 0.0                               \n",
        "  for batch_index, batch_dict in enumerate(batch_generator):\n",
        "      optimizer.zero_grad()\n",
        "      # print(\"batch_index\",batch_index)\n",
        "      y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "      # print(\"y_pred=\",np.array(y_pred).shape)\n",
        "\n",
        "      # step 3. compute the loss\n",
        "      loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "      loss_t = loss.item()\n",
        "      running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "      # step 4. use loss to produce gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # step 5. use optimizer to take gradient step\n",
        "      optimizer.step()\n",
        "      # -----------------------------------------\n",
        "      # compute the accuracy\n",
        "      # acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "      # running_acc += (acc_t - running_acc) / (batch_index + 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3H8gCfHCrbNn"
      },
      "source": [
        "dataset.set_split('val')\n",
        "batch_generator = generate_batches(dataset, batch_size=args.batch_size, device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "\n",
        "    # compute the output\n",
        "    y_pred = classifier(x_in=batch_dict['x_data'].float())\n",
        "\n",
        "    # step 3. compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'].float())\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    # acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    # running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0KevwQtsW3B"
      },
      "source": [
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsTECwxWtRt7"
      },
      "source": [
        "  # def vectorize(self, review, review_vocab):\n",
        "  #     print(\"review=\",review)\n",
        "  #     one_hot = np.zeros(review_vocab, dtype=np.float32)\n",
        "  #     # print(\"one_hot.size\",one_hot.shape)#306\n",
        "  #     for token in review.split(\" \"):\n",
        "  #       if token not in string.punctuation:#從字去找位置idx\n",
        "  #           one_hot[self.review_lookup_token(token)] = 1#在一句話中無法考慮相同的字出現次數 只會在該位置=1只會在該位置=1\n",
        "  #     # print(\"one_hot.shape=\",one_hot.shape)\n",
        "  #     return one_hot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LDEl2m4saHp"
      },
      "source": [
        "def predict_rating(review, classifier, dataset, decision_threshold=0.5):\n",
        "    \"\"\"Predict the rating of a review\n",
        "    \n",
        "    Args:\n",
        "        review (str): the text of the review\n",
        "        classifier (ReviewClassifier): the trained model\n",
        "        vectorizer (ReviewVectorizer): the corresponding vectorizer\n",
        "        decision_threshold (float): The numerical boundary which separates the rating classes\n",
        "    \"\"\"\n",
        "    review = preprocess_text(review)\n",
        "    \n",
        "    vectorized_review = torch.tensor(dataset.vectorize(review, dataset.review_vocab))#one hot\n",
        "    print(\"vectorized_review.view(1, -1)=\",vectorized_review.view(1, -1))\n",
        "    result = classifier(vectorized_review.view(1, -1))\n",
        "    print(\"result=\",result)\n",
        "    \n",
        "    probability_value = F.sigmoid(result).item()\n",
        "    print(\"probability_value=\",probability_value)\n",
        "    index = 1\n",
        "    if probability_value < decision_threshold:\n",
        "        index = 0\n",
        "    print(\"index=\",index)\n",
        "    print(dataset.rating_lookup_index(index))\n",
        "    return dataset.rating_lookup_index(index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYiBYE93scjo",
        "outputId": "ad73944d-72ea-44f7-e6d8-d30f35c26d97"
      },
      "source": [
        "test_review = \"this is a pretty awesome book\"\n",
        "dataset.set_split('train')\n",
        "classifier = classifier.cpu()\n",
        "prediction = predict_rating(test_review, classifier, dataset, decision_threshold=0.5)\n",
        "print(\"{} -> {}\".format(test_review, prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "self._target_df\n",
            "        rating                                             review  split\n",
            "0     negative  overpriced food . they were out of a lot of co...  train\n",
            "1     negative  we had been hearing rave reviews about this pi...  train\n",
            "2     negative  needing a food fix while walking through vario...  train\n",
            "3     negative  walmart makes me cringe . not only for all kin...  train\n",
            "4     negative  we love this place , granted our first time wa...  train\n",
            "...        ...                                                ...    ...\n",
            "3225  positive  wow the pasta is really good at this place . t...  train\n",
            "3226  positive  they re a great local alternative to starbucks...  train\n",
            "3227  positive  i wish there was a way i could give this place...  train\n",
            "3228  positive  went there yesterday and found that the place ...  train\n",
            "3229  positive  all in all , this is one of my fave spots to g...  train\n",
            "\n",
            "[2660 rows x 3 columns]\n",
            "self._target_size= 2660\n",
            "review= this is a pretty awesome book\n",
            "vectorized_review.view(1, -1)= tensor([[1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "         0.]])\n",
            "result= tensor(1.1875, grad_fn=<SqueezeBackward0>)\n",
            "probability_value= 0.7662855982780457\n",
            "index= 1\n",
            "positive\n",
            "this is a pretty awesome book -> positive\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1639: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jy9xtzpknzxF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "5e912a9c-f71a-48c4-b79a-c28d35160a96"
      },
      "source": [
        "  vectorizer_filepath='model_storage/ch3/yelp/'+'vectorizer.json'\n",
        "  with open(vectorizer_filepath, \"w\") as fp:\n",
        "    json.dump(self._vectorizer.to_serializable(), fp)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-48d04e7ec629>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer_filepath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'model_storage/ch3/yelp/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'vectorizer.json'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer_filepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_serializable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    }
  ]
}