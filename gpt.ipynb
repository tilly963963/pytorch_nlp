{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gpt.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPCc1G2iNfCjkdCiDHUE2TG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tilly963963/pytorch_nlp/blob/main/gpt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2N-2dl2zJiZ2"
      },
      "source": [
        "https://www.cnblogs.com/wwj99/p/12503545.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVSFiJ2tqBJq"
      },
      "source": [
        "import random\n",
        "\n",
        "def select_top_k(predictions, k=10):\n",
        "    predicted_index = random.choice(\n",
        "        predictions[0, -1, :].sort(descending=True)[1][:10]).item()\n",
        "    print(\"predicted_index=\",predicted_index)\n",
        "    return predicted_index"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L9Jl5F-YqMhV",
        "outputId": "d1ac1c49-ef18-4d4f-c588-0746ccf34b3c"
      },
      "source": [
        "!pip install pytorch_transformers==1.0  # 安装 PyTorch-Transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch_transformers==1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/b5/2d78e74001af0152ee61d5ad4e290aec9a1e43925b21df2dc74ec100f1ab/pytorch_transformers-1.0.0-py3-none-any.whl (137kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 13.7MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/11/20/4294e37c3c6936c905f1e9da958c776d7fee54a4512bdb7706d69c8720e6/boto3-1.17.84-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 23.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers==1.0) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers==1.0) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers==1.0) (1.19.5)\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 23.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers==1.0) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch_transformers==1.0) (4.41.1)\n",
            "Collecting botocore<1.21.0,>=1.20.84\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/22/72c81d754bbcb128cba2ad88670c3c320e4594e6ddd8cca6512c3967108c/botocore-1.20.84-py2.py3-none-any.whl (7.6MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6MB 38.4MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers==1.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers==1.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers==1.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch_transformers==1.0) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch_transformers==1.0) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.84->boto3->pytorch_transformers==1.0) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.84->boto3->pytorch_transformers==1.0) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.84 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, sentencepiece, pytorch-transformers\n",
            "Successfully installed boto3-1.17.84 botocore-1.20.84 jmespath-0.10.0 pytorch-transformers-1.0.0 s3transfer-0.4.2 sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9nTLfrsqPbx",
        "outputId": "dddecc3e-78f6-4f41-e29b-65c7e96323c6"
      },
      "source": [
        "import torch\n",
        "from pytorch_transformers import GPT2Tokenizer\n",
        "\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "# 载入预训练模型的分词器\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "# 使用 GPT2Tokenizer 对输入进行编码\n",
        "text = \"the ct images were reviewed by two radiologists who reached decisions by consensus. we included only patients who presented with abnormalities on ct and in whom the diagnosis had been confirmed by pathological examination of the surgical specimen (if the lesion was resected). the ct scans were assessed in order to identify the main findings and to map the distribution of the lesions (i.e., to determine whether the pulmonary involvement was unilateral or bilateral). results: the main ct finding was the combination\"\n",
        "indexed_tokens = tokenizer.encode(text)\n",
        "tokens_tensor = torch.tensor([indexed_tokens])\n",
        "tokens_tensor.shape\n",
        "print(tokens_tensor)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /root/.cache/torch/pytorch_transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
            "INFO:pytorch_transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /root/.cache/torch/pytorch_transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 1169,   269,    83,  4263,   547, 11765,   416,   734, 19772,  9251,\n",
            "           508,  4251,  5370,   416, 11529,    13,   356,  3017,   691,  3871,\n",
            "           508,  5545,   351, 34615,   319,   269,    83,   290,   287,  4150,\n",
            "           262, 13669,   550,   587,  4999,   416, 42306, 12452,   286,   262,\n",
            "         21998, 31674,   357,   361,   262, 10287,   295,   373,  1599,   310,\n",
            "           276,   737,   262,   269,    83, 23824,   547, 15276,   287,  1502,\n",
            "           284,  5911,   262,  1388,  6373,   290,   284,  3975,   262,  6082,\n",
            "           286,   262, 35258,   357,    72,    13,    68,  1539,   284,  5004,\n",
            "          1771,   262, 45105,  9750,   373, 37095,   393, 24537,   737,  2482,\n",
            "            25,   262,  1388,   269,    83,  4917,   373,   262,  6087]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZO4hA8qSqTNB",
        "outputId": "712f0db7-c34b-4823-83bf-de7620fdda32"
      },
      "source": [
        "from pytorch_transformers import GPT2LMHeadModel\n",
        "\n",
        "# 读取 GPT-2 预训练模型\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "total_predicted_text = text\n",
        "n = 100  # 预测过程的循环次数\n",
        "for _ in range(n):\n",
        "    with torch.no_grad():\n",
        "        outputs = model(tokens_tensor)\n",
        "        predictions = outputs[0]\n",
        "\n",
        "    predicted_index = select_top_k(predictions, k=10)\n",
        "    predicted_text = tokenizer.decode(indexed_tokens + [predicted_index])\n",
        "    total_predicted_text += tokenizer.decode(predicted_index)\n",
        "\n",
        "    if '<|endoftext|>' in total_predicted_text:\n",
        "        # 如果出现文本结束标志，就结束文本生成\n",
        "        break\n",
        "\n",
        "    indexed_tokens += [predicted_index]\n",
        "    tokens_tensor = torch.tensor([indexed_tokens])\n",
        "\n",
        "print(total_predicted_text)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_transformers.modeling_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /root/.cache/torch/pytorch_transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.db13c9bc9c7bdd738ec89e069621d88e05dc670366092d809a9cbcac6798e24e\n",
            "INFO:pytorch_transformers.modeling_utils:Model config {\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"GPT2LMHeadModel\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.1,\n",
            "  \"bos_token_id\": 50256,\n",
            "  \"embd_pdrop\": 0.1,\n",
            "  \"eos_token_id\": 50256,\n",
            "  \"finetuning_task\": null,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"gpt2\",\n",
            "  \"n_ctx\": 1024,\n",
            "  \"n_embd\": 768,\n",
            "  \"n_head\": 12,\n",
            "  \"n_layer\": 12,\n",
            "  \"n_positions\": 1024,\n",
            "  \"num_labels\": 1,\n",
            "  \"output_attentions\": false,\n",
            "  \"output_hidden_states\": false,\n",
            "  \"resid_pdrop\": 0.1,\n",
            "  \"summary_activation\": null,\n",
            "  \"summary_first_dropout\": 0.1,\n",
            "  \"summary_proj_to_labels\": true,\n",
            "  \"summary_type\": \"cls_index\",\n",
            "  \"summary_use_proj\": true,\n",
            "  \"task_specific_params\": {\n",
            "    \"text-generation\": {\n",
            "      \"do_sample\": true,\n",
            "      \"max_length\": 50\n",
            "    }\n",
            "  },\n",
            "  \"torchscript\": false,\n",
            "  \"vocab_size\": 50257\n",
            "}\n",
            "\n",
            "INFO:pytorch_transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /root/.cache/torch/pytorch_transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "the ct images were reviewed by two radiologists who reached decisions by consensus. we included only patients who presented with abnormalities on ct and in whom the diagnosis had been confirmed by pathological examination of the surgical specimen (if the lesion was resected). the ct scans were assessed in order to identify the main findings and to map the distribution of the lesions (i.e., to determine whether the pulmonary involvement was unilateral or bilateral). results: the main ct finding was the combination between bilateral hemorrhage with the pulmonary lesions as the dominant site of disease was associated strongly with hemorrhages with greater frequency compared With this study only 2 out 3 lesions were associated significantly with a bilateral pulmonary pathology and only 6 were statistically significantly significant and the rest did show some overlap. we found significant associations (p value < 10-6 and 0%, p >.005 in Fig 5) among the 3 sites of disease where hemorrhagic events had been associated. The other 6 showed little association at all and\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}