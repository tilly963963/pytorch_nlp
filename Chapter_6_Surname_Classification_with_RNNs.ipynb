{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "nlpbook",
      "language": "python",
      "name": "nlpbook"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "toc": {
      "colors": {
        "hover_highlight": "#DAA520",
        "running_highlight": "#FF0000",
        "selected_highlight": "#FFD700"
      },
      "moveMenuLeft": true,
      "nav_menu": {
        "height": "120px",
        "width": "252px"
      },
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": "5",
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Chapter-6-Surname-Classification-with-RNNs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tilly963963/pytorch_nlp/blob/main/Chapter_6_Surname_Classification_with_RNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCEgzHLx8ucy",
        "outputId": "7eafe16c-3c57-4ad8-eabb-bda5c64cedcd"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFNQwCht85tV"
      },
      "source": [
        "import os\n",
        "os.chdir(\"/content/drive/Shareddrives/python教學/PyTorchNLPBook-master/chapters/chapter_6/classifying-surnames\")"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LtAouC6b3hzX"
      },
      "source": [
        "# Surname Classification \n",
        "\n",
        "In this example, we see surname classification. \n",
        "\n",
        "This is a modified version of the main example in which we use the PackedSequences data structure that PyTorch provides.  While PackedSequences is a useful data structure, seeing what's happening with column indexing is very useful. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NR0Ki21D3hzd"
      },
      "source": [
        "from argparse import Namespace\n",
        "import os\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "import time"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAFb4SkW9SeI"
      },
      "source": [
        "#隨手練習"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjv2AUf-3bDE",
        "outputId": "0c5b0337-a754-4c0f-d745-24b3637ffaee"
      },
      "source": [
        "def listToTensor():\n",
        "    tensor1=torch.tensor([1,2,3])\n",
        "    tensor2=torch.tensor([4,5,6])\n",
        "    tensor_list=list()\n",
        "\n",
        "    tensor_list.append(tensor1)\n",
        "    tensor_list.append(tensor2)\n",
        "    final_tensor=torch.stack(tensor_list)\n",
        "    print('tensor_list:',tensor_list, 'type:',type(tensor_list))\n",
        "    print('final_tensor:',final_tensor, 'type',type(final_tensor))\n",
        "    tensor_list_v2=list()\n",
        "\n",
        "    tensor1=[torch.tensor([1,2,3])]\n",
        "    tensor2=[torch.tensor([4,5,6])]\n",
        "    # tensor_list_v2.append(tensor1)\n",
        "    tensor_1 = torch.stack(tensor1)\n",
        "    print(\"tensor_1=\",tensor_1)\n",
        "    tensor_2 = torch.stack(tensor2)\n",
        "    cat_tensor = torch.cat((tensor_1, tensor_2))\n",
        "    print(\"cat_tensor=\",cat_tensor)\n",
        "\n",
        "\n",
        "listToTensor()\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor_list: [tensor([1, 2, 3]), tensor([4, 5, 6])] type: <class 'list'>\n",
            "final_tensor: tensor([[1, 2, 3],\n",
            "        [4, 5, 6]]) type <class 'torch.Tensor'>\n",
            "tensor_1= tensor([[1, 2, 3]])\n",
            "cat_tensor= tensor([[1, 2, 3],\n",
            "        [4, 5, 6]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoMwtPw13hze"
      },
      "source": [
        "## Vocabulary, Vectorizer, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxiGHsCd3hze"
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Class to process text and extract vocabulary for mapping\"\"\"\n",
        "\n",
        "    def __init__(self, token_to_idx=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            token_to_idx (dict): a pre-existing map of tokens to indices\n",
        "        \"\"\"\n",
        "        print(\"nationality_vocab Vocabulary __init__\")\n",
        "        if token_to_idx is None:\n",
        "            token_to_idx = {}\n",
        "        self._token_to_idx = token_to_idx\n",
        "\n",
        "        self._idx_to_token = {idx: token \n",
        "                              for token, idx in self._token_to_idx.items()}\n",
        "        print(\"self._token_to_idx=\",self._token_to_idx)\n",
        "        print(\"self._idx_to_token=\",self._idx_to_token)         \n",
        "    def to_serializable(self):\n",
        "        \"\"\" returns a dictionary that can be serialized \"\"\"\n",
        "        return {'token_to_idx': self._token_to_idx}\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        \"\"\" instantiates the Vocabulary from a serialized dictionary \"\"\"\n",
        "        return cls(**contents)\n",
        "\n",
        "    def add_token(self, token):\n",
        "        \"\"\"Update mapping dicts based on the token.\n",
        "\n",
        "        Args:\n",
        "            token (str): the item to add into the Vocabulary\n",
        "        Returns:\n",
        "            index (int): the integer corresponding to the token\n",
        "        \"\"\"\n",
        "        if token in self._token_to_idx:\n",
        "            index = self._token_to_idx[token]\n",
        "        else:\n",
        "            index = len(self._token_to_idx)\n",
        "            self._token_to_idx[token] = index\n",
        "            self._idx_to_token[index] = token\n",
        "        # print(token,\":\",index)\n",
        "        # time.sleep(10)\n",
        "        return index\n",
        "            \n",
        "    def add_many(self, tokens):\n",
        "        \"\"\"Add a list of tokens into the Vocabulary\n",
        "        \n",
        "        Args:\n",
        "            tokens (list): a list of string tokens\n",
        "        Returns:\n",
        "            indices (list): a list of indices corresponding to the tokens\n",
        "        \"\"\"\n",
        "        return [self.add_token(token) for token in tokens]\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token \n",
        "        \n",
        "        Args:\n",
        "            token (str): the token to look up \n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        \"\"\"\n",
        "        return self._token_to_idx[token]\n",
        "\n",
        "    def lookup_index(self, index):\n",
        "        \"\"\"Return the token associated with the index\n",
        "        \n",
        "        Args: \n",
        "            index (int): the index to look up\n",
        "        Returns:\n",
        "            token (str): the token corresponding to the index\n",
        "        Raises:\n",
        "            KeyError: if the index is not in the Vocabulary\n",
        "        \"\"\"\n",
        "        if index not in self._idx_to_token:\n",
        "            raise KeyError(\"the index (%d) is not in the Vocabulary\" % index)\n",
        "        return self._idx_to_token[index]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"<Vocabulary(size=%d)>\" % len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._token_to_idx)"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xS8eNaU3hzf"
      },
      "source": [
        "class SequenceVocabulary(Vocabulary):\n",
        "    def __init__(self, token_to_idx=None, unk_token=\"<UNK>\",\n",
        "                 mask_token=\"<MASK>\", begin_seq_token=\"<BEGIN>\",\n",
        "                 end_seq_token=\"<END>\"):\n",
        "        print(\"char_vocab SequenceVocabulary __init__\")\n",
        "        super(SequenceVocabulary, self).__init__(token_to_idx)\n",
        "\n",
        "        self._mask_token = mask_token\n",
        "        self._unk_token = unk_token\n",
        "        self._begin_seq_token = begin_seq_token\n",
        "        self._end_seq_token = end_seq_token\n",
        "\n",
        "        self.mask_index = self.add_token(self._mask_token)#'<MASK>': 0\n",
        "        self.unk_index = self.add_token(self._unk_token)#'<UNK>': 1\n",
        "        self.begin_seq_index = self.add_token(self._begin_seq_token)#'<BEGIN>': 2\n",
        "        self.end_seq_index = self.add_token(self._end_seq_token)#'<END>': 3\n",
        "\n",
        "    def to_serializable(self):\n",
        "        contents = super(SequenceVocabulary, self).to_serializable()\n",
        "        contents.update({'unk_token': self._unk_token,\n",
        "                         'mask_token': self._mask_token,\n",
        "                         'begin_seq_token': self._begin_seq_token,\n",
        "                         'end_seq_token': self._end_seq_token})\n",
        "        print(\"contents=\",contents)\n",
        "        # contents= {'token_to_idx': {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'T': 4, 'o': 5, 't': 6, 'a': 7, 'h': 8, 'A': 9, 'b': 10, 'u': 11, 'd': 12, 'F': 13, 'k': 14, 'r': 15, 'y': 16, 'S': 17, 'e': 18, 'g': 19, 'C': 20, 'm': 21, 'H': 22, 'i': 23, 'K': 24, 'n': 25, 'W': 26, 's': 27, 'f': 28, 'G': 29, 'M': 30, 'l': 31, 'B': 32, 'z': 33, 'N': 34, 'I': 35, 'w': 36, 'D': 37, 'Q': 38, 'j': 39, 'E': 40, 'R': 41, 'Z': 42, 'c': 43, 'Y': 44, 'J': 45, 'L': 46, 'O': 47, '-': 48, 'P': 49, 'X': 50, 'p': 51, ':': 52, 'v': 53, 'U': 54, '1': 55, 'V': 56, 'x': 57, 'q': 58, 'é': 59, 'É': 60, \"'\": 61, 'ß': 62, 'ö': 63, 'ä': 64, 'ü': 65, 'ú': 66, 'à': 67, 'ò': 68, 'è': 69, 'ó': 70, 'Ś': 71, 'ą': 72, 'ń': 73, 'á': 74, 'ż': 75, 'õ': 76, 'í': 77, 'ñ': 78, 'Á': 79},\n",
        "              #  'unk_token': '<UNK>', \n",
        "              # 'mask_token': '<MASK>', \n",
        "              # 'begin_seq_token': '<BEGIN>', \n",
        "              # 'end_seq_token': '<END>'}\n",
        "        return contents\n",
        "\n",
        "    def lookup_token(self, token):\n",
        "        \"\"\"Retrieve the index associated with the token \n",
        "          or the UNK index if token isn't present.\n",
        "        \n",
        "        Args:\n",
        "            token (str): the token to look up \n",
        "        Returns:\n",
        "            index (int): the index corresponding to the token\n",
        "        Notes:\n",
        "            `unk_index` needs to be >=0 (having been added into the Vocabulary) \n",
        "              for the UNK functionality \n",
        "        \"\"\"\n",
        "        if self.unk_index >= 0:\n",
        "            return self._token_to_idx.get(token, self.unk_index)\n",
        "        else:\n",
        "            return self._token_to_idx[token]"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF0WAlyV3hzf"
      },
      "source": [
        "class SurnameVectorizer(object):\n",
        "    \"\"\" The Vectorizer which coordinates the Vocabularies and puts them to use\"\"\"   \n",
        "    def __init__(self, char_vocab, nationality_vocab):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            char_vocab (Vocabulary): maps characters to integers\n",
        "            nationality_vocab (Vocabulary): maps nationalities to integers\n",
        "        \"\"\"\n",
        "        print(\"SurnameVectorizer __init__\")\n",
        "        self.char_vocab = char_vocab\n",
        "        self.nationality_vocab = nationality_vocab\n",
        "\n",
        "    def vectorize(self, surname, vector_length=-1):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            title (str): the string of characters\n",
        "            vector_length (int): an argument for forcing the length of index vector\n",
        "        \"\"\"\n",
        "        indices = [self.char_vocab.begin_seq_index]#2 '<BEGIN>': 2\n",
        "        indices.extend(self.char_vocab.lookup_token(token) \n",
        "                       for token in surname)\n",
        "        indices.append(self.char_vocab.end_seq_index)\n",
        "\n",
        "        if vector_length < 0:\n",
        "            vector_length = len(indices)\n",
        "\n",
        "        out_vector = np.zeros(vector_length, dtype=np.int64)         \n",
        "        out_vector[:len(indices)] = indices\n",
        "        out_vector[len(indices):] = self.char_vocab.mask_index#0 '<MASK>': 0\n",
        "        # print(\"out_vector=\",out_vector)\n",
        "        # out_vector= [2 4 5 6 7 8 3 0 0 0 0 0 0 0 0 0 0 0 0] len(indices)=7\n",
        "        return out_vector, len(indices)\n",
        "\n",
        "    @classmethod\n",
        "    def from_dataframe(cls, surname_df):\n",
        "        \"\"\"Instantiate the vectorizer from the dataset dataframe\n",
        "        \n",
        "        Args:\n",
        "            surname_df (pandas.DataFrame): the surnames dataset\n",
        "        Returns:\n",
        "            an instance of the SurnameVectorizer\n",
        "        \"\"\"\n",
        "        print(\"SurnameVectorizer from_dataframe()\")\n",
        "        char_vocab = SequenceVocabulary()\n",
        "        # char_vocab SequenceVocabulary __init__\n",
        "          # super(SequenceVocabulary, self).__init__(token_to_idx) nationality_vocab Vocabulary __init__\n",
        "        # self._token_to_idx= {}\n",
        "        # self._idx_to_token= {}\n",
        "        # <MASK> : 0\n",
        "        # <UNK> : 1\n",
        "        # <BEGIN> : 2\n",
        "        # <END> : 3\n",
        "        \n",
        "        nationality_vocab = Vocabulary()\n",
        "        # nationality_vocab Vocabulary __init__\n",
        "        # self._token_to_idx= {}\n",
        "        # self._idx_to_token= {}\n",
        "\n",
        "        for index, row in surname_df.iterrows():\n",
        "            for char in row.surname:\n",
        "                char_vocab.add_token(char)\n",
        "                # T : 4\n",
        "                # o : 5\n",
        "                # t : 6\n",
        "                # a : 7\n",
        "                # h : 8\n",
        "            nationality_vocab.add_token(row.nationality)\n",
        "            # Arabic : 0\n",
        "            if index == 0:\n",
        "              print(\"row=\",row)\n",
        "              print(\"char_vocab=\",char_vocab)\n",
        "              print(\"nationality_vocab=\",nationality_vocab)\n",
        "#row\n",
        "# nationality          Arabic\n",
        "# nationality_index        15\n",
        "# split                 train\n",
        "# surname               Totah\n",
        "# Name: 0, dtype: object\n",
        "\n",
        "# char_vocab= <Vocabulary(size=9)>\n",
        "# nationality_vocab= <Vocabulary(size=1)>\n",
        "        return cls(char_vocab, nationality_vocab)\n",
        "\n",
        "    @classmethod\n",
        "    def from_serializable(cls, contents):\n",
        "        char_vocab = SequenceVocabulary.from_serializable(contents['char_vocab'])\n",
        "        nat_vocab =  Vocabulary.from_serializable(contents['nationality_vocab'])\n",
        "\n",
        "        return cls(char_vocab=char_vocab, nationality_vocab=nat_vocab)\n",
        "\n",
        "    def to_serializable(self):\n",
        "        return {'char_vocab': self.char_vocab.to_serializable(), \n",
        "                'nationality_vocab': self.nationality_vocab.to_serializable()}"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYnajUgx3hzg"
      },
      "source": [
        "import time\n",
        "class SurnameDataset(Dataset):\n",
        "    def __init__(self, surname_df, vectorizer):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            surname_df (pandas.DataFrame): the dataset\n",
        "            vectorizer (SurnameVectorizer): vectorizer instatiated from dataset\n",
        "        \"\"\"\n",
        "        self.surname_df = surname_df \n",
        "        self._vectorizer = vectorizer\n",
        "        print(\"map(len, self.surname_df.surname)=\",list(map(len, self.surname_df.surname)))\n",
        "        # map(len, self.surname_df.surname)= [5, 6, 8, 5, 6, 4...\n",
        "        # nums= [1,-2,-3]\n",
        "        # ans = map(abs,nums) #abs是內建函數，取絕對值的意思\n",
        "        # print(list(ans)) #用list()將迭代器轉回正常的列表\n",
        "        self._max_seq_length = max(map(len, self.surname_df.surname)) + 2\n",
        "        print(\"self._max_seq_length=\",self._max_seq_length)#19\n",
        "        self.train_df = self.surname_df[self.surname_df.split=='train']\n",
        "        self.train_size = len(self.train_df)\n",
        "\n",
        "        self.val_df = self.surname_df[self.surname_df.split=='val']\n",
        "        self.validation_size = len(self.val_df)\n",
        "\n",
        "        self.test_df = self.surname_df[self.surname_df.split=='test']\n",
        "        self.test_size = len(self.test_df)\n",
        "\n",
        "        self._lookup_dict = {'train': (self.train_df, self.train_size), \n",
        "                             'val': (self.val_df, self.validation_size), \n",
        "                             'test': (self.test_df, self.test_size)}\n",
        "\n",
        "        self.set_split('train')\n",
        "        \n",
        "        # Class weights\n",
        "        class_counts = self.train_df.nationality.value_counts().to_dict()\n",
        "        print(\"class_counts=\",class_counts)#計算數量\n",
        "        def sort_key(item):\n",
        "            # print(\"item[0]=\",item[0],\":\",self._vectorizer.nationality_vocab.lookup_token(item[0]))\n",
        "            # item[0]= English : 4\n",
        "            # item[0]= Russian : 14\n",
        "            # item[0]= Arabic : 0\n",
        "            return self._vectorizer.nationality_vocab.lookup_token(item[0])\n",
        "        print(\"class_counts.items()=\",class_counts.items())\n",
        "        sorted_counts = sorted(class_counts.items(), key=sort_key)\n",
        "        print(\"sorted_counts=\",sorted_counts)#依照文本順序 index 排序\n",
        "        frequencies = [count for _, count in sorted_counts]\n",
        "        print(\"frequencies=\",frequencies)\n",
        "        self.class_weights = 1.0 / torch.tensor(frequencies, dtype=torch.float32)\n",
        "\n",
        "# class_counts= {'English': 2080, 'Russian': 1661, 'Arabic': 1122, 'Japanese': 542, 'Italian': 420, 'German': 403, \n",
        "# 'Czech': 289, 'Spanish': 180, 'Dutch': 165, 'French': 160, 'Chinese': 154, 'Irish': 128, 'Greek': 109, 'Polish': 84,\n",
        "#  'Korean': 53, 'Scottish': 52, 'Vietnamese': 40, 'Portuguese': 38}\n",
        "\n",
        "#class_counts.items()= dict_items([('English', 2080), ('Russian', 1661), ('Arabic', 1122), ('Japanese', 542), ('Italian', 420), ('German', 403), ('Czech', 289), ('Spanish', 180), ('Dutch', 165), ('French', 160), ('Chinese', 154), ('Irish', 128), ('Greek', 109), ('Polish', 84), ('Korean', 53), ('Scottish', 52), ('Vietnamese', 40), ('Portuguese', 38)])\n",
        "# sorted_counts= [('Arabic', 1122), ('Chinese', 154), ('Czech', 289), ('Dutch', 165), ('English', 2080), ('French', 160), ('German', 403), ('Greek', 109), ('Irish', 128), ('Italian', 420), ('Japanese', 542), ('Korean', 53), ('Polish', 84), ('Portuguese', 38), ('Russian', 1661), ('Scottish', 52), ('Spanish', 180), ('Vietnamese', 40)]\n",
        "# frequencies= [1122, 154, 289, 165, 2080, 160, 403, 109, 128, 420, 542, 53, 84, 38, 1661, 52, 180, 40]\n",
        "# contents= {'token_to_idx': {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'T': 4, 'o': 5, 't': 6, 'a': 7, 'h': 8, 'A': 9, 'b': 10, 'u': 11, 'd': 12, 'F': 13, 'k': 14, 'r': 15, 'y': 16, 'S': 17, 'e': 18, 'g': 19, 'C': 20, 'm': 21, 'H': 22, 'i': 23, 'K': 24, 'n': 25, 'W': 26, 's': 27, 'f': 28, 'G': 29, 'M': 30, 'l': 31, 'B': 32, 'z': 33, 'N': 34, 'I': 35, 'w': 36, 'D': 37, 'Q': 38, 'j': 39, 'E': 40, 'R': 41, 'Z': 42, 'c': 43, 'Y': 44, 'J': 45, 'L': 46, 'O': 47, '-': 48, 'P': 49, 'X': 50, 'p': 51, ':': 52, 'v': 53, 'U': 54, '1': 55, 'V': 56, 'x': 57, 'q': 58, 'é': 59, 'É': 60, \"'\": 61, 'ß': 62, 'ö': 63, 'ä': 64, 'ü': 65, 'ú': 66, 'à': 67, 'ò': 68, 'è': 69, 'ó': 70, 'Ś': 71, 'ą': 72, 'ń': 73, 'á': 74, 'ż': 75, 'õ': 76, 'í': 77, 'ñ': 78, 'Á': 79}, \n",
        "#            'unk_token': '<UNK>', 'mask_token': '<MASK>', 'begin_seq_token': '<BEGIN>', 'end_seq_token': '<END>'}\n",
        "        \n",
        "    @classmethod\n",
        "    def load_dataset_and_make_vectorizer(cls, surname_csv):\n",
        "        \"\"\"Load dataset and make a new vectorizer from scratch\n",
        "        \n",
        "        Args:\n",
        "            surname_csv (str): location of the dataset\n",
        "        Returns:\n",
        "            an instance of SurnameDataset\n",
        "        \"\"\"\n",
        "        surname_df = pd.read_csv(surname_csv)\n",
        "        train_surname_df = surname_df[surname_df.split=='train']\n",
        "        print(\"train_surname_df=\")\n",
        "        print(train_surname_df)\n",
        "#         train_surname_df=\n",
        "#       nationality  nationality_index  split   surname\n",
        "# 0          Arabic                 15  train     Totah\n",
        "# 1          Arabic                 15  train    Abboud\n",
        "# 2          Arabic                 15  train  Fakhoury\n",
        "# 3          Arabic                 15  train     Srour\n",
        "# 4          Arabic                 15  train    Sayegh\n",
        "# ...           ...                ...    ...       ...\n",
        "# 10957  Vietnamese                 11  train       Chu\n",
        "# 10958  Vietnamese                 11  train      Pham\n",
        "# 10959  Vietnamese                 11  train      Chau\n",
        "# 10960  Vietnamese                 11  train      Mach\n",
        "# 10961  Vietnamese                 11  train      Tieu\n",
        "\n",
        "# [7680 rows x 4 columns]\n",
        "        return cls(surname_df, SurnameVectorizer.from_dataframe(train_surname_df))\n",
        "        \n",
        "    @classmethod\n",
        "    def load_dataset_and_load_vectorizer(cls, surname_csv, vectorizer_filepath):\n",
        "        \"\"\"Load dataset and the corresponding vectorizer. \n",
        "        Used in the case in the vectorizer has been cached for re-use\n",
        "        \n",
        "        Args:\n",
        "            surname_csv (str): location of the dataset\n",
        "            vectorizer_filepath (str): location of the saved vectorizer\n",
        "        Returns:\n",
        "            an instance of SurnameDataset\n",
        "        \"\"\"\n",
        "        surname_df = pd.read_csv(surname_csv)\n",
        "        vectorizer = cls.load_vectorizer_only(vectorizer_filepath)\n",
        "        return cls(surname_df, vectorizer)\n",
        "\n",
        "    @staticmethod\n",
        "    def load_vectorizer_only(vectorizer_filepath):\n",
        "        \"\"\"a static method for loading the vectorizer from file\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location of the serialized vectorizer\n",
        "        Returns:\n",
        "            an instance of SurnameVectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath) as fp:\n",
        "            return SurnameVectorizer.from_serializable(json.load(fp))\n",
        "\n",
        "    def save_vectorizer(self, vectorizer_filepath):\n",
        "        \"\"\"saves the vectorizer to disk using json\n",
        "        \n",
        "        Args:\n",
        "            vectorizer_filepath (str): the location to save the vectorizer\n",
        "        \"\"\"\n",
        "        with open(vectorizer_filepath, \"w\") as fp:\n",
        "            json.dump(self._vectorizer.to_serializable(), fp)\n",
        "\n",
        "    def get_vectorizer(self):\n",
        "        \"\"\" returns the vectorizer \"\"\"\n",
        "        print(\"get_vectorizer()\")\n",
        "        print(\"self._vectorizer=\",self._vectorizer)\n",
        "        return self._vectorizer\n",
        "\n",
        "    def set_split(self, split=\"train\"):\n",
        "        self._target_split = split\n",
        "        self._target_df, self._target_size = self._lookup_dict[split]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self._target_size\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\"the primary entry point method for PyTorch datasets\n",
        "        \n",
        "        Args:\n",
        "            index (int): the index to the data point \n",
        "        Returns:\n",
        "            a dictionary holding the data point's:\n",
        "                features (x_data)\n",
        "                label (y_target)\n",
        "                feature length (x_length)\n",
        "        \"\"\"\n",
        "        row = self._target_df.iloc[index]\n",
        "        \n",
        "        surname_vector, vec_length = \\\n",
        "            self._vectorizer.vectorize(row.surname, self._max_seq_length)\n",
        "        print(\"surname_vector=\",surname_vector,\"vec_length=\",vec_length)\n",
        "        nationality_index = \\\n",
        "            self._vectorizer.nationality_vocab.lookup_token(row.nationality)\n",
        "        print(\"nationality_index=\",nationality_index)\n",
        "        # out_vector= (19,)\n",
        "        # surname_vector= [2 4 5 6 7 8 3 0 0 0 0 0 0 0 0 0 0 0 0] vec_length= 7 Totah\n",
        "        # nationality_index= 0 Arabic(15)\n",
        "        # out_vector= (19,)\n",
        "        # surname_vector= [ 2  9 10 10  5 11 12  3  0  0  0  0  0  0  0  0  0  0  0] vec_length= 8\n",
        "        # nationality_index= 0\n",
        "\n",
        "        return {'x_data': surname_vector, \n",
        "                'y_target': nationality_index, \n",
        "                'x_length': vec_length}\n",
        "\n",
        "    def get_num_batches(self, batch_size):\n",
        "        \"\"\"Given a batch size, return the number of batches in the dataset\n",
        "        \n",
        "        Args:\n",
        "            batch_size (int)\n",
        "        Returns:\n",
        "            number of batches in the dataset\n",
        "        \"\"\"\n",
        "        return len(self) // batch_size\n",
        "\n",
        "    \n",
        "\n",
        "def generate_batches(dataset, batch_size, shuffle=True,\n",
        "                     drop_last=True, device=\"cpu\"): \n",
        "    \"\"\"\n",
        "    A generator function which wraps the PyTorch DataLoader. It will \n",
        "      ensure each tensor is on the write device location.\n",
        "    \"\"\"\n",
        "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n",
        "                            shuffle=False, drop_last=drop_last)\n",
        "    for data_dict in dataloader:\n",
        "        '''\n",
        "        當迭代dataloader時會自動呼叫 \n",
        "        class DataLoader(object)  \n",
        "          def __iter__(self): \n",
        "            return DataLoaderIter(self)\n",
        "            \n",
        "        class DataLoaderIter(object): \n",
        "          def __next__(self):\n",
        "            batch = self.collate_fn([self.dataset[i] for i in indices])\n",
        "            self.dataset[i] => __getitem__() 每一次回傳批次個  用self.collate_fn 打包成批次個\n",
        "        '''\n",
        "        # out_vector= (19,)\n",
        "        # surname_vector= [2 4 5 6 7 8 3 0 0 0 0 0 0 0 0 0 0 0 0] vec_length= 7 Totah\n",
        "        # nationality_index= 0 Arabic(15)\n",
        "\n",
        "        # out_vector= (19,)\n",
        "        # surname_vector= [ 2  9 10 10  5 11 12  3  0  0  0  0  0  0  0  0  0  0  0] vec_length= 8\n",
        "        # nationality_index= 0\n",
        "\n",
        "        # data_dict= {'x_data': surname_vector, \n",
        "        #         'y_target': nationality_index, \n",
        "        #         'x_length': vec_length}\n",
        "\n",
        "        out_data_dict = {}\n",
        "        for name, tensor in data_dict.items():\n",
        "            out_data_dict[name] = data_dict[name].to(device)\n",
        "            # time.sleep(10)\n",
        "        yield out_data_dict"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSCfLbRG3hzj"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1JMP8YX3hzl"
      },
      "source": [
        "\n",
        "def column_gather(y_out, x_lengths):\n",
        "    '''Get a specific vector from each batch datapoint in `y_out`.\n",
        "\n",
        "    More precisely, iterate over batch row indices, get the vector that's at\n",
        "    the position indicated by the corresponding value in `x_lengths` at the row\n",
        "    index.\n",
        "\n",
        "    Args:\n",
        "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
        "            shape: (batch, sequence, feature)## print(\"y_out.shape\",y_out.shape)#torch.Size([2, 19, 64])\n",
        "        x_lengths (torch.LongTensor, torch.cuda.LongTensor)\n",
        "            shape: (batch,)\n",
        "\n",
        "    Returns:\n",
        "        y_out (torch.FloatTensor, torch.cuda.FloatTensor)\n",
        "            shape: (batch, feature)\n",
        "    '''\n",
        "    print(\"column_gather\")\n",
        "    '''\n",
        "    只需取最後一個數字的隱藏h (排除填補的0)\n",
        "    '''\n",
        "    x_lengths = x_lengths.long().detach().cpu().numpy() - 1\n",
        "    print(\"x_lengths=\",x_lengths)#x_lengths= [6 7] 每個批次內的字元長度\n",
        "    out = []\n",
        "    for batch_index, column_index in enumerate(x_lengths):\n",
        "        out.append(y_out[batch_index, column_index])#每個批次內最後一個字元的h\n",
        "    print(\"np.array(out).shape=\",np.array(out).shape)\n",
        "    # print(out)\n",
        "    # np.array(out).shape= (2,)\n",
        "    # [tensor([-0.9011, ... -0.8600],device='cuda:0', grad_fn=<SelectBackward>), tensor([-0.9481, ..., -0.2505],\n",
        "    #        device='cuda:0', grad_fn=<SelectBackward>)]\n",
        "    return torch.stack(out)\n",
        "    # y_out\n",
        "    # tensor([[-0.9011, ..., -0.8600],[-0.9481,  ..., -0.2505]],device='cuda:0', grad_fn=<StackBackward>)\n",
        "    ## torch.Size([2, 64])\n",
        "\n",
        "class ElmanRNN(nn.Module):\n",
        "    \"\"\" an Elman RNN built using the RNNCell \"\"\"\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            input_size (int): size of the input vectors\n",
        "            hidden_size (int): size of the hidden state vectors\n",
        "            bathc_first (bool): whether the 0th dimension is batch\n",
        "        \"\"\"\n",
        "        super(ElmanRNN, self).__init__()\n",
        "        \n",
        "        self.rnn_cell = nn.RNNCell(input_size, hidden_size)\n",
        "        \n",
        "        self.batch_first = batch_first\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def _initial_hidden(self, batch_size):\n",
        "        return torch.zeros((batch_size, self.hidden_size))\n",
        "\n",
        "    def forward(self, x_in, initial_hidden=None):\n",
        "        \"\"\"The forward pass of the ElmanRNN\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                If self.batch_first: x_in.shape = (batch, seq_size, feat_size)\n",
        "                Else: x_in.shape = (seq_size, batch, feat_size)\n",
        "            initial_hidden (torch.Tensor): the initial hidden state for the RNN\n",
        "        Returns:\n",
        "            hiddens (torch.Tensor): The outputs of the RNN at each time step. \n",
        "                If self.batch_first: hiddens.shape = (batch, seq_size, hidden_size)\n",
        "                Else: hiddens.shape = (seq_size, batch, hidden_size)\n",
        "        \"\"\"\n",
        "\n",
        "        print(\"self.batch_first=\",self.batch_first)# self.batch_first= True\n",
        "        if self.batch_first:\n",
        "            batch_size, seq_size, feat_size = x_in.size() #torch.Size([2, 19, 100])\n",
        "            x_in = x_in.permute(1, 0, 2)\n",
        "        else:\n",
        "            seq_size, batch_size, feat_size = x_in.size()\n",
        "        \n",
        "        # x_in torch.Size([19, 2, 100])\n",
        "        hiddens = []\n",
        "\n",
        "        if initial_hidden is None:\n",
        "            initial_hidden = self._initial_hidden(batch_size)#b,h\n",
        "            initial_hidden = initial_hidden.to(x_in.device)\n",
        "\n",
        "        hidden_t = initial_hidden\n",
        "                    \n",
        "        for t in range(seq_size):#19\n",
        "            print(\"t=\",t,\"x_in[t]=\",x_in[t].shape,\"hidden_t=\",hidden_t.shape)\n",
        "            hidden_t = self.rnn_cell(x_in[t], hidden_t)\n",
        "            hiddens.append(hidden_t)\n",
        "            '''\n",
        "            t= 0 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])#initial_hidden\n",
        "            t= 1 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])#t-1時刻的h產出\n",
        "            t= 2 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 3 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 4 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 5 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 6 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 7 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 8 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 9 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 10 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 11 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 12 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 13 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 14 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 15 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 16 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 17 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            t= 18 x_in[t]= torch.Size([2, 100]) hidden_t= torch.Size([2, 64])\n",
        "            '''\n",
        "        print(\"hiddens.shape=\",np.array(hiddens).shape)# (19,)\n",
        "        # print(hiddens) \n",
        "        # [tensor([[-0.4455, ... -0.5285],[-0.4455, ... -0.5285]],device='cuda:0', grad_fn=<TanhBackward>), \n",
        "        # ...\n",
        "        # tensor([[-8.6683e-0... -3.3850e-01],[-8.6578e-02, ... -3.3991e-01]], device='cuda:0', grad_fn=<TanhBackward>)]        \n",
        "\n",
        "        hiddens = torch.stack(hiddens)\n",
        "        print(\"hiddens.shape torch =\",hiddens.shape)#torch.Size([19, 2, 64])  \n",
        "        # print(hiddens)\n",
        "        # tensor([[[-0.4455,  0.9444, -0.1651,  ..., -0.3475, -0.1790, -0.5285],\n",
        "        #       [-0.4455,  0.9444, -0.1651,  ..., -0.3475, -0.1790, -0.5285]],\n",
        "        # ...\n",
        "        # [[-0.0867, -0.1494, -0.0267,  ...,  0.0855,  0.1793, -0.3385],\n",
        "        #   [-0.0866, -0.1501, -0.0279,  ...,  0.0857,  0.1800, -0.3399]]],\n",
        "        # device='cuda:0', grad_fn=<StackBackward>)        \n",
        "\n",
        "        if self.batch_first:\n",
        "            hiddens = hiddens.permute(1, 0, 2)## print(\"y_out.shape\",y_out.shape)#torch.Size([2, 19, 64])\n",
        "     \n",
        "      \n",
        "\n",
        "        return hiddens\n",
        "\n",
        "\n",
        "\n",
        "class SurnameClassifier(nn.Module):\n",
        "    \"\"\" A Classifier with an RNN to extract features and an MLP to classify \"\"\"\n",
        "    def __init__(self, embedding_size, num_embeddings, num_classes,\n",
        "                 rnn_hidden_size, batch_first=True, padding_idx=0):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            embedding_size (int): The size of the character embeddings\n",
        "            num_embeddings (int): The number of characters to embed\n",
        "            num_classes (int): The size of the prediction vector \n",
        "                Note: the number of nationalities\n",
        "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
        "            batch_first (bool): Informs whether the input tensors will \n",
        "                have batch or the sequence on the 0th dimension\n",
        "            padding_idx (int): The index for the tensor padding; \n",
        "                see torch.nn.Embedding\n",
        "        \"\"\"\n",
        "        # classifier = SurnameClassifier(embedding_size=args.char_embedding_size, #100\n",
        "        #         num_embeddings=len(vectorizer.char_vocab),#80\n",
        "        #         num_classes=len(vectorizer.nationality_vocab),#18\n",
        "        #         rnn_hidden_size=args.rnn_hidden_size,#64\n",
        "        #         padding_idx=vectorizer.char_vocab.mask_index)#0\n",
        "\n",
        "        super(SurnameClassifier, self).__init__()\n",
        "\n",
        "        self.emb = nn.Embedding(num_embeddings=num_embeddings,#80\n",
        "                                embedding_dim=embedding_size,#100\n",
        "                                padding_idx=padding_idx)#0\n",
        "        self.rnn = ElmanRNN(input_size=embedding_size,\n",
        "                             hidden_size=rnn_hidden_size,\n",
        "                             batch_first=batch_first)\n",
        "        self.fc1 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                         out_features=rnn_hidden_size)\n",
        "        self.fc2 = nn.Linear(in_features=rnn_hidden_size,\n",
        "                          out_features=num_classes)\n",
        "\n",
        "    def forward(self, x_in, x_lengths=None, apply_softmax=False):\n",
        "        \"\"\"The forward pass of the classifier\n",
        "        \n",
        "        Args:\n",
        "            x_in (torch.Tensor): an input data tensor. \n",
        "                x_in.shape should be (batch, input_dim)\n",
        "            x_lengths (torch.Tensor): the lengths of each sequence in the batch.\n",
        "                They are used to find the final vector of each sequence\n",
        "            apply_softmax (bool): a flag for the softmax activation\n",
        "                should be false if used with the Cross Entropy losses\n",
        "        Returns:\n",
        "            the resulting tensor. tensor.shape should be (batch, output_dim)\n",
        "        \"\"\"\n",
        "        print(\"x_in.shape=\",x_in.shape)#x_in.shape= torch.Size([2, 19])\n",
        "\n",
        "        x_embedded = self.emb(x_in)\n",
        "        # print(\"x_embedded.shape=\",x_embedded.shape)#torch.Size([2, 19, 100])\n",
        "        y_out = self.rnn(x_embedded)\n",
        "        # print(\"y_out.shape\",y_out.shape)#torch.Size([2, 19, 64])\n",
        "\n",
        "        if x_lengths is not None:\n",
        "            y_out = column_gather(y_out, x_lengths)#最後一個字的h\n",
        "            # print(\"x_lengths=\",x_lengths)#tensor([7, 8], device='cuda:0')\n",
        "            # print(\"y_out_2.shape=\",y_out.shape)#torch.Size([2, 64])\n",
        "            print(\"y_out\")\n",
        "            print(y_out)\n",
        "        else:\n",
        "            y_out = y_out[:, -1, :]\n",
        "\n",
        "        y_out = F.relu(self.fc1(F.dropout(y_out, 0.5)))\n",
        "        # print(\"y_out_3.shape\",y_out.shape)#torch.Size([2, 64])\n",
        "        \n",
        "        y_out = self.fc2(F.dropout(y_out, 0.5))\n",
        "        # print(\"y_out_4.shape\",y_out.shape)#torch.Size([2, 18])\n",
        "\n",
        "        if apply_softmax:\n",
        "            y_out = F.softmax(y_out, dim=1)\n",
        "        # print(\"y_out_5.shape\",y_out.shape)#torch.Size([2, 18]) \n",
        "        \n",
        "        return y_out"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpLyz_tF3hzl"
      },
      "source": [
        "def set_seed_everywhere(seed, cuda):\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if cuda:\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def handle_dirs(dirpath):\n",
        "    if not os.path.exists(dirpath):\n",
        "        os.makedirs(dirpath)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYjJUvdR3hzl"
      },
      "source": [
        "## Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arGlD78m3hzm",
        "outputId": "84cf2abb-db69-4f25-dd47-c9157af951df"
      },
      "source": [
        "args = Namespace(\n",
        "    # Data and path information\n",
        "    surname_csv=\"data/surnames/surnames_with_splits.csv\",\n",
        "    vectorizer_file=\"vectorizer.json\",\n",
        "    model_state_file=\"model.pth\",\n",
        "    save_dir=\"model_storage/ch6/surname_classification\",\n",
        "    # Model hyper parameter\n",
        "    char_embedding_size=100,\n",
        "    rnn_hidden_size=64,\n",
        "    # Training hyper parameter\n",
        "    num_epochs=100,\n",
        "    learning_rate=1e-3,\n",
        "    batch_size=2,\n",
        "    seed=1337,\n",
        "    early_stopping_criteria=5,\n",
        "    # Runtime hyper parameter\n",
        "    cuda=True,\n",
        "    catch_keyboard_interrupt=True,\n",
        "    reload_from_files=False,\n",
        "    expand_filepaths_to_save_dir=True,\n",
        ")\n",
        "\n",
        "# Check CUDA\n",
        "if not torch.cuda.is_available():\n",
        "    args.cuda = False\n",
        "\n",
        "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
        "    \n",
        "print(\"Using CUDA: {}\".format(args.cuda))\n",
        "\n",
        "\n",
        "if args.expand_filepaths_to_save_dir:\n",
        "    args.vectorizer_file = os.path.join(args.save_dir, args.vectorizer_file)\n",
        "\n",
        "    args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n",
        "    \n",
        "# Set seed for reproducibility\n",
        "set_seed_everywhere(args.seed, args.cuda)\n",
        "\n",
        "# handle dirs\n",
        "handle_dirs(args.save_dir)"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using CUDA: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [
          0,
          4
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GzXuKuH93hzm",
        "outputId": "4db5d389-2f04-4a1e-e199-465d59b2e5ee"
      },
      "source": [
        "if args.reload_from_files and os.path.exists(args.vectorizer_file):\n",
        "    # training from a checkpoint\n",
        "    dataset = SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv, args.vectorizer_file)\n",
        "else:\n",
        "    # create dataset and vectorizer\n",
        "    dataset = SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
        "    dataset.save_vectorizer(args.vectorizer_file)\n",
        "#contents={'token_to_idx': {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'T': 4, 'o': 5, 't': 6, 'a': 7, 'h': 8, '\n",
        "# A': 9, 'b': 10, 'u': 11, 'd': 12, 'F': 13, 'k': 14, 'r': 15, 'y': 16, 'S': 17, 'e': 18, 'g': 19, 'C': 20, 'm': 21, \n",
        "# 'H': 22, 'i': 23, 'K': 24, 'n': 25, 'W': 26, 's': 27, 'f': 28, 'G': 29, 'M': 30, 'l': 31, 'B': 32, 'z': 33, 'N': 34,\n",
        "#  'I': 35, 'w': 36, 'D': 37, 'Q': 38, 'j': 39, 'E': 40, 'R': 41, 'Z': 42, 'c': 43, 'Y': 44, 'J': 45, 'L': 46,\n",
        "#  'O': 47, '-': 48, 'P': 49, 'X': 50, 'p': 51, ':': 52, 'v': 53, 'U': 54, '1': 55, 'V': 56, 'x': 57, 'q': 58, \n",
        "# 'é': 59, 'É': 60,\"'\": 61, 'ß': 62, 'ö': 63, 'ä': 64, 'ü': 65, 'ú': 66, 'à': 67, 'ò': 68, 'è': 69, 'ó': 70, \n",
        "# 'Ś': 71, 'ą': 72, 'ń': 73, 'á': 74, 'ż': 75, 'õ': 76, 'í': 77, 'ñ': 78, 'Á': 79},\n",
        "#  'unk_token': '<UNK>', 'mask_token': '<MASK>', 'begin_seq_token': '<BEGIN>', 'end_seq_token': '<END>'}\n",
        "vectorizer = dataset.get_vectorizer()\n",
        "\n",
        "classifier = SurnameClassifier(embedding_size=args.char_embedding_size, #100\n",
        "                num_embeddings=len(vectorizer.char_vocab),#80\n",
        "                num_classes=len(vectorizer.nationality_vocab),#18\n",
        "                rnn_hidden_size=args.rnn_hidden_size,#64\n",
        "                padding_idx=vectorizer.char_vocab.mask_index)#0"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train_surname_df=\n",
            "      nationality  nationality_index  split   surname\n",
            "0          Arabic                 15  train     Totah\n",
            "1          Arabic                 15  train    Abboud\n",
            "2          Arabic                 15  train  Fakhoury\n",
            "3          Arabic                 15  train     Srour\n",
            "4          Arabic                 15  train    Sayegh\n",
            "...           ...                ...    ...       ...\n",
            "10957  Vietnamese                 11  train       Chu\n",
            "10958  Vietnamese                 11  train      Pham\n",
            "10959  Vietnamese                 11  train      Chau\n",
            "10960  Vietnamese                 11  train      Mach\n",
            "10961  Vietnamese                 11  train      Tieu\n",
            "\n",
            "[7680 rows x 4 columns]\n",
            "SurnameVectorizer from_dataframe()\n",
            "char_vocab SequenceVocabulary __init__\n",
            "nationality_vocab Vocabulary __init__\n",
            "self._token_to_idx= {}\n",
            "self._idx_to_token= {}\n",
            "nationality_vocab Vocabulary __init__\n",
            "self._token_to_idx= {}\n",
            "self._idx_to_token= {}\n",
            "row= nationality          Arabic\n",
            "nationality_index        15\n",
            "split                 train\n",
            "surname               Totah\n",
            "Name: 0, dtype: object\n",
            "char_vocab= <Vocabulary(size=9)>\n",
            "nationality_vocab= <Vocabulary(size=1)>\n",
            "SurnameVectorizer __init__\n",
            "map(len, self.surname_df.surname)= [5, 6, 8, 5, 6, 4, 4, 6, 6, 6, 5, 5, 4, 8, 6, 4, 6, 7, 6, 5, 5, 3, 5, 5, 3, 4, 4, 6, 8, 6, 8, 7, 7, 7, 5, 7, 7, 5, 5, 6, 5, 6, 8, 8, 4, 7, 5, 4, 3, 6, 6, 5, 6, 5, 6, 7, 5, 6, 8, 8, 7, 5, 5, 5, 6, 6, 4, 4, 4, 7, 7, 4, 5, 6, 5, 4, 6, 4, 5, 5, 5, 6, 5, 5, 4, 6, 5, 5, 5, 3, 5, 4, 5, 6, 7, 7, 5, 7, 6, 6, 4, 6, 5, 6, 7, 7, 4, 7, 6, 5, 6, 5, 6, 6, 4, 4, 7, 5, 5, 6, 4, 6, 6, 4, 8, 5, 5, 6, 5, 6, 5, 7, 6, 6, 6, 5, 5, 5, 4, 4, 6, 6, 7, 4, 6, 4, 5, 6, 5, 6, 6, 4, 6, 5, 6, 6, 6, 6, 5, 6, 4, 5, 6, 5, 5, 3, 5, 4, 7, 4, 7, 4, 6, 4, 6, 6, 6, 8, 7, 3, 5, 8, 6, 5, 6, 6, 6, 5, 3, 7, 5, 6, 3, 4, 8, 5, 4, 4, 4, 7, 4, 6, 5, 6, 7, 6, 4, 5, 7, 6, 7, 5, 6, 6, 5, 6, 5, 6, 6, 6, 6, 5, 4, 8, 6, 6, 4, 7, 6, 5, 6, 6, 6, 5, 6, 7, 5, 6, 6, 7, 5, 5, 5, 4, 5, 8, 6, 6, 5, 5, 6, 6, 6, 6, 5, 5, 6, 4, 6, 6, 4, 7, 6, 7, 8, 6, 7, 4, 5, 6, 6, 5, 5, 5, 6, 7, 6, 5, 5, 5, 4, 6, 4, 5, 5, 5, 5, 6, 5, 5, 5, 6, 6, 6, 5, 6, 6, 4, 5, 7, 6, 5, 4, 5, 6, 6, 5, 7, 3, 6, 4, 3, 8, 4, 6, 6, 5, 5, 6, 6, 7, 6, 4, 6, 7, 6, 6, 6, 6, 4, 5, 6, 6, 6, 6, 8, 6, 4, 5, 5, 6, 5, 8, 6, 8, 6, 8, 7, 4, 6, 6, 6, 4, 6, 6, 6, 6, 5, 8, 2, 6, 5, 7, 6, 7, 5, 4, 6, 6, 5, 5, 6, 5, 7, 7, 8, 5, 8, 6, 6, 5, 5, 5, 4, 6, 5, 4, 5, 6, 5, 6, 6, 6, 6, 6, 6, 5, 6, 6, 6, 7, 6, 5, 6, 7, 6, 5, 6, 7, 8, 6, 4, 4, 4, 4, 6, 6, 7, 4, 5, 6, 6, 5, 6, 6, 6, 7, 5, 6, 5, 5, 5, 5, 5, 5, 4, 6, 5, 5, 5, 4, 6, 6, 4, 4, 5, 4, 8, 6, 7, 4, 6, 6, 6, 5, 6, 6, 5, 5, 4, 5, 5, 7, 6, 6, 5, 5, 7, 6, 3, 7, 7, 4, 6, 5, 3, 6, 7, 5, 5, 6, 8, 7, 8, 4, 6, 6, 5, 6, 4, 5, 5, 4, 5, 5, 6, 3, 6, 5, 4, 5, 6, 5, 5, 4, 6, 3, 7, 6, 6, 5, 6, 5, 4, 4, 7, 6, 6, 6, 5, 6, 5, 7, 5, 5, 6, 6, 5, 5, 6, 4, 6, 3, 4, 7, 5, 7, 6, 6, 6, 5, 7, 3, 6, 5, 7, 6, 6, 6, 5, 5, 5, 4, 6, 6, 5, 6, 6, 7, 5, 7, 5, 5, 4, 5, 7, 6, 5, 7, 5, 6, 4, 4, 6, 6, 7, 5, 5, 4, 6, 7, 5, 5, 5, 7, 5, 6, 6, 5, 5, 4, 6, 6, 6, 6, 8, 8, 5, 5, 7, 8, 6, 4, 3, 8, 5, 7, 5, 5, 6, 8, 8, 6, 5, 5, 5, 7, 6, 7, 6, 7, 7, 5, 7, 5, 7, 6, 4, 5, 5, 4, 6, 5, 6, 5, 8, 6, 4, 5, 6, 6, 7, 5, 5, 5, 6, 5, 6, 6, 5, 6, 6, 6, 5, 6, 5, 7, 6, 6, 5, 4, 6, 5, 6, 5, 5, 8, 5, 5, 5, 5, 7, 6, 6, 7, 6, 5, 6, 5, 6, 7, 2, 6, 5, 6, 6, 5, 6, 5, 5, 6, 6, 6, 5, 6, 5, 6, 6, 7, 7, 6, 6, 8, 4, 6, 5, 6, 5, 7, 5, 7, 6, 6, 4, 6, 5, 5, 5, 6, 7, 6, 4, 5, 4, 5, 5, 5, 4, 3, 4, 7, 6, 6, 6, 5, 4, 6, 5, 7, 4, 6, 6, 4, 6, 5, 5, 7, 4, 6, 7, 5, 8, 6, 8, 5, 6, 6, 5, 5, 5, 5, 6, 5, 7, 5, 6, 4, 5, 6, 6, 4, 3, 5, 4, 7, 5, 5, 5, 5, 8, 5, 8, 6, 4, 8, 6, 6, 5, 6, 7, 5, 8, 5, 3, 6, 5, 7, 8, 5, 6, 5, 6, 6, 6, 5, 6, 6, 5, 5, 7, 4, 5, 7, 5, 5, 6, 5, 6, 5, 4, 4, 5, 6, 6, 4, 6, 5, 5, 5, 7, 6, 5, 6, 4, 5, 6, 2, 7, 6, 6, 6, 7, 6, 6, 6, 6, 5, 6, 5, 5, 7, 7, 5, 6, 7, 6, 5, 7, 4, 5, 6, 6, 5, 5, 6, 5, 5, 5, 6, 7, 6, 7, 6, 5, 8, 7, 7, 4, 6, 6, 6, 5, 5, 6, 7, 5, 6, 7, 5, 5, 5, 6, 8, 6, 6, 3, 6, 6, 2, 7, 6, 4, 5, 6, 6, 8, 7, 7, 6, 6, 4, 4, 6, 6, 7, 6, 5, 5, 6, 8, 4, 7, 6, 5, 5, 6, 5, 5, 6, 6, 5, 8, 6, 6, 5, 4, 6, 6, 4, 6, 5, 6, 5, 6, 6, 5, 4, 5, 4, 6, 5, 5, 4, 4, 7, 6, 7, 5, 5, 8, 4, 5, 6, 6, 6, 5, 5, 5, 6, 5, 3, 7, 6, 4, 5, 5, 5, 5, 5, 7, 5, 7, 5, 6, 5, 4, 4, 6, 5, 5, 6, 7, 6, 4, 7, 6, 8, 7, 4, 6, 6, 5, 7, 5, 5, 5, 6, 6, 5, 5, 6, 5, 6, 6, 4, 6, 4, 6, 5, 5, 6, 3, 7, 5, 7, 6, 7, 5, 6, 5, 7, 5, 6, 6, 5, 5, 4, 5, 6, 6, 7, 4, 8, 6, 6, 6, 6, 7, 4, 6, 6, 6, 5, 4, 4, 6, 4, 3, 7, 4, 8, 6, 4, 5, 4, 4, 5, 5, 6, 6, 5, 6, 6, 4, 6, 5, 6, 4, 8, 6, 5, 5, 5, 5, 6, 8, 4, 5, 5, 5, 5, 5, 6, 6, 4, 5, 6, 4, 5, 6, 4, 7, 6, 5, 3, 7, 7, 6, 5, 6, 6, 6, 5, 6, 6, 4, 6, 6, 6, 4, 6, 6, 5, 5, 7, 7, 4, 6, 7, 6, 5, 6, 7, 6, 7, 5, 4, 6, 5, 6, 8, 6, 6, 7, 4, 5, 7, 6, 6, 5, 6, 4, 4, 6, 5, 5, 8, 6, 7, 7, 7, 6, 6, 4, 4, 6, 8, 6, 5, 6, 5, 6, 6, 6, 5, 8, 8, 6, 6, 4, 5, 5, 7, 7, 4, 6, 7, 7, 6, 5, 5, 6, 6, 5, 8, 5, 5, 6, 5, 6, 7, 7, 4, 5, 6, 4, 5, 6, 5, 6, 6, 5, 6, 5, 5, 4, 5, 5, 5, 4, 8, 6, 6, 2, 6, 7, 6, 6, 5, 4, 7, 6, 4, 5, 7, 6, 5, 5, 6, 5, 5, 7, 7, 5, 5, 6, 4, 6, 4, 5, 7, 8, 2, 6, 5, 5, 8, 6, 4, 5, 6, 6, 8, 5, 6, 5, 6, 5, 7, 7, 5, 4, 4, 5, 5, 6, 5, 4, 5, 6, 6, 8, 6, 4, 6, 6, 6, 6, 6, 6, 5, 7, 5, 6, 6, 6, 5, 5, 8, 4, 6, 6, 6, 7, 7, 5, 7, 5, 4, 6, 7, 5, 6, 5, 6, 7, 6, 6, 6, 7, 5, 5, 6, 5, 4, 4, 6, 6, 5, 4, 8, 7, 5, 6, 7, 5, 6, 6, 5, 5, 6, 7, 7, 5, 6, 6, 6, 7, 7, 4, 7, 8, 4, 5, 5, 4, 5, 6, 5, 6, 6, 6, 7, 7, 5, 5, 6, 4, 5, 4, 4, 6, 6, 5, 6, 7, 6, 7, 4, 6, 5, 4, 6, 5, 3, 5, 6, 4, 6, 5, 5, 5, 5, 6, 5, 6, 6, 6, 5, 4, 6, 6, 5, 6, 6, 6, 6, 4, 5, 4, 4, 6, 4, 6, 5, 3, 7, 6, 6, 5, 7, 5, 5, 5, 6, 4, 5, 6, 3, 5, 4, 7, 5, 5, 7, 5, 6, 5, 4, 8, 5, 4, 5, 6, 6, 4, 6, 4, 6, 6, 5, 7, 8, 6, 4, 6, 5, 6, 5, 7, 6, 6, 5, 4, 7, 6, 6, 5, 7, 2, 8, 6, 5, 7, 5, 7, 5, 6, 5, 3, 5, 6, 2, 5, 7, 6, 7, 6, 6, 6, 6, 5, 5, 6, 6, 6, 7, 5, 5, 5, 6, 5, 7, 4, 5, 6, 6, 5, 6, 8, 6, 5, 6, 5, 7, 6, 6, 6, 6, 5, 5, 6, 5, 6, 4, 6, 7, 7, 5, 4, 6, 6, 6, 6, 6, 6, 6, 7, 8, 6, 6, 6, 5, 6, 5, 4, 5, 6, 6, 6, 5, 4, 5, 3, 8, 7, 7, 5, 4, 6, 6, 7, 7, 6, 4, 5, 5, 6, 6, 6, 6, 7, 6, 6, 3, 6, 6, 6, 6, 5, 5, 6, 6, 6, 5, 5, 4, 6, 6, 5, 4, 7, 7, 6, 5, 4, 6, 8, 5, 5, 4, 8, 6, 3, 4, 3, 4, 3, 4, 3, 4, 3, 3, 3, 3, 3, 4, 4, 3, 3, 3, 5, 3, 3, 5, 3, 3, 3, 3, 5, 4, 3, 4, 5, 3, 4, 3, 3, 7, 3, 4, 4, 4, 3, 4, 5, 5, 3, 4, 3, 4, 3, 3, 4, 7, 3, 4, 3, 4, 3, 5, 5, 3, 4, 3, 3, 4, 4, 3, 3, 3, 4, 3, 5, 5, 3, 4, 5, 4, 3, 3, 4, 5, 4, 4, 3, 3, 3, 3, 3, 4, 3, 3, 4, 3, 3, 3, 3, 7, 5, 3, 4, 4, 3, 5, 3, 3, 4, 3, 3, 4, 3, 4, 3, 4, 3, 4, 3, 3, 4, 4, 3, 4, 5, 3, 4, 5, 4, 4, 3, 3, 5, 3, 4, 3, 3, 3, 4, 4, 4, 3, 3, 3, 4, 4, 4, 3, 3, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4, 4, 4, 3, 3, 4, 4, 3, 4, 4, 4, 3, 4, 3, 4, 5, 4, 4, 3, 3, 6, 3, 4, 4, 3, 4, 4, 4, 4, 3, 5, 4, 4, 3, 5, 4, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 4, 3, 4, 3, 3, 3, 3, 3, 3, 4, 4, 5, 3, 8, 6, 5, 6, 9, 7, 7, 5, 9, 6, 7, 5, 3, 4, 8, 9, 7, 10, 5, 3, 5, 7, 7, 7, 7, 5, 8, 5, 9, 4, 7, 6, 9, 7, 5, 10, 5, 8, 7, 6, 4, 5, 6, 4, 5, 4, 12, 8, 7, 4, 10, 9, 7, 7, 5, 9, 5, 6, 5, 5, 5, 5, 8, 5, 7, 7, 6, 6, 5, 8, 6, 6, 7, 6, 6, 7, 6, 5, 9, 6, 7, 5, 4, 7, 9, 6, 8, 5, 6, 8, 6, 5, 3, 10, 8, 7, 5, 7, 8, 6, 7, 8, 8, 6, 6, 5, 10, 9, 5, 8, 6, 7, 6, 6, 6, 4, 5, 3, 4, 6, 6, 6, 8, 6, 5, 9, 7, 5, 6, 5, 7, 5, 6, 6, 9, 7, 4, 5, 7, 5, 6, 6, 3, 7, 6, 6, 9, 8, 5, 7, 5, 6, 10, 5, 7, 13, 6, 8, 12, 6, 6, 4, 7, 6, 6, 7, 5, 7, 4, 5, 8, 7, 4, 9, 6, 6, 4, 8, 7, 6, 5, 6, 7, 7, 7, 6, 7, 9, 7, 6, 5, 5, 8, 6, 6, 8, 4, 6, 7, 6, 5, 7, 5, 9, 10, 8, 8, 6, 6, 4, 5, 8, 5, 8, 5, 6, 6, 5, 6, 8, 7, 11, 6, 5, 6, 8, 8, 5, 4, 6, 10, 7, 7, 8, 9, 7, 7, 7, 5, 9, 5, 8, 7, 9, 5, 9, 5, 13, 7, 5, 6, 9, 5, 7, 7, 6, 5, 9, 12, 6, 6, 4, 7, 5, 5, 6, 4, 8, 7, 7, 5, 7, 6, 9, 5, 6, 6, 7, 9, 12, 7, 5, 5, 6, 8, 6, 7, 4, 7, 5, 9, 6, 5, 5, 4, 10, 5, 6, 8, 5, 12, 7, 9, 6, 6, 6, 6, 7, 6, 10, 6, 5, 7, 6, 7, 7, 6, 8, 6, 7, 6, 6, 7, 6, 9, 6, 4, 6, 6, 7, 7, 6, 6, 5, 6, 6, 6, 4, 5, 9, 5, 5, 4, 7, 6, 9, 5, 7, 7, 7, 5, 5, 9, 7, 9, 5, 4, 5, 6, 7, 7, 6, 7, 4, 9, 6, 8, 7, 10, 5, 6, 6, 6, 5, 5, 6, 10, 5, 7, 5, 6, 7, 6, 5, 6, 11, 7, 4, 7, 3, 5, 10, 8, 6, 6, 4, 7, 7, 7, 6, 4, 7, 7, 4, 6, 5, 7, 5, 7, 9, 8, 5, 6, 6, 4, 6, 9, 4, 6, 7, 7, 6, 6, 5, 5, 4, 6, 10, 7, 9, 7, 5, 7, 6, 9, 8, 5, 10, 8, 7, 4, 5, 6, 8, 4, 8, 9, 8, 7, 5, 5, 9, 5, 8, 6, 7, 10, 4, 4, 10, 7, 6, 7, 9, 7, 6, 10, 6, 5, 8, 7, 5, 4, 5, 3, 4, 5, 8, 9, 6, 11, 7, 7, 8, 9, 6, 8, 4, 7, 6, 7, 5, 10, 6, 8, 6, 9, 7, 6, 11, 4, 3, 7, 12, 7, 7, 10, 8, 4, 8, 16, 7, 4, 7, 7, 5, 6, 7, 6, 7, 5, 9, 7, 6, 7, 14, 3, 7, 5, 9, 6, 10, 8, 5, 5, 5, 7, 6, 12, 4, 10, 7, 8, 5, 8, 6, 6, 10, 9, 8, 8, 6, 5, 7, 8, 5, 5, 7, 4, 10, 8, 10, 6, 9, 7, 6, 8, 3, 7, 8, 5, 5, 5, 7, 9, 11, 3, 11, 7, 11, 6, 12, 5, 5, 9, 6, 7, 5, 8, 8, 9, 13, 4, 10, 5, 7, 7, 7, 6, 6, 8, 5, 6, 9, 4, 6, 4, 6, 7, 5, 9, 7, 5, 7, 7, 8, 10, 8, 7, 10, 7, 7, 7, 4, 6, 5, 8, 7, 6, 8, 6, 8, 4, 8, 9, 7, 8, 3, 6, 6, 4, 8, 7, 8, 8, 9, 12, 4, 10, 9, 4, 6, 8, 6, 6, 5, 4, 5, 5, 5, 5, 7, 7, 6, 5, 7, 8, 5, 6, 5, 4, 5, 6, 7, 8, 7, 9, 6, 8, 6, 6, 6, 7, 6, 5, 5, 7, 6, 4, 6, 5, 6, 6, 7, 5, 6, 6, 7, 8, 6, 6, 6, 10, 6, 9, 6, 6, 5, 4, 5, 9, 8, 7, 7, 8, 6, 7, 5, 8, 6, 9, 9, 6, 9, 6, 5, 6, 7, 9, 6, 7, 4, 7, 5, 7, 7, 7, 10, 5, 5, 9, 6, 7, 4, 6, 4, 5, 8, 4, 5, 6, 8, 6, 6, 5, 7, 7, 5, 6, 6, 4, 6, 6, 6, 6, 7, 7, 5, 6, 7, 5, 6, 7, 5, 7, 7, 4, 5, 7, 6, 7, 5, 4, 6, 6, 5, 5, 7, 5, 7, 8, 5, 5, 11, 5, 5, 8, 5, 6, 4, 5, 5, 7, 6, 7, 4, 6, 6, 5, 6, 5, 5, 5, 7, 6, 7, 6, 7, 6, 5, 8, 7, 6, 7, 6, 5, 5, 9, 4, 6, 9, 7, 6, 6, 5, 8, 8, 7, 6, 8, 8, 7, 6, 5, 5, 4, 4, 7, 6, 6, 5, 6, 7, 8, 7, 6, 7, 6, 8, 9, 4, 4, 6, 5, 4, 8, 7, 8, 4, 8, 5, 6, 7, 10, 9, 5, 3, 7, 5, 5, 8, 7, 5, 7, 6, 9, 6, 7, 5, 7, 5, 5, 6, 6, 7, 4, 6, 4, 7, 8, 6, 7, 10, 6, 4, 6, 4, 8, 7, 5, 9, 6, 7, 6, 9, 7, 6, 7, 5, 7, 6, 7, 8, 6, 5, 6, 9, 5, 8, 7, 6, 5, 6, 9, 9, 9, 7, 5, 7, 6, 7, 6, 5, 5, 6, 6, 7, 6, 5, 5, 7, 7, 5, 6, 6, 5, 9, 6, 7, 4, 7, 6, 4, 9, 5, 6, 7, 7, 6, 7, 6, 4, 8, 6, 6, 5, 6, 5, 4, 8, 6, 7, 8, 5, 5, 7, 10, 7, 8, 8, 8, 6, 6, 10, 5, 5, 8, 6, 4, 5, 5, 6, 7, 6, 6, 6, 7, 6, 5, 5, 4, 5, 8, 8, 5, 4, 5, 7, 6, 7, 6, 4, 4, 6, 9, 4, 8, 8, 5, 5, 7, 7, 4, 5, 4, 5, 5, 5, 5, 7, 9, 6, 5, 4, 8, 5, 4, 4, 5, 5, 7, 4, 7, 9, 6, 10, 6, 10, 5, 6, 5, 8, 9, 6, 6, 6, 4, 5, 6, 8, 5, 7, 8, 7, 5, 8, 8, 5, 9, 5, 5, 7, 6, 5, 5, 5, 7, 6, 8, 7, 4, 6, 8, 8, 6, 5, 7, 4, 7, 6, 7, 5, 6, 8, 10, 4, 7, 5, 8, 6, 10, 6, 6, 6, 6, 8, 6, 8, 9, 9, 8, 7, 5, 4, 6, 6, 4, 7, 7, 8, 5, 7, 8, 4, 7, 6, 5, 4, 7, 6, 4, 6, 6, 6, 9, 5, 6, 7, 5, 5, 6, 8, 6, 4, 7, 6, 6, 9, 7, 5, 4, 6, 7, 5, 8, 8, 6, 7, 8, 4, 5, 5, 4, 6, 6, 8, 4, 7, 7, 6, 5, 6, 4, 9, 6, 7, 8, 7, 7, 5, 9, 7, 5, 7, 4, 6, 4, 6, 8, 5, 8, 7, 5, 5, 4, 6, 6, 7, 6, 9, 6, 6, 8, 6, 7, 6, 8, 4, 4, 8, 4, 6, 6, 6, 5, 6, 7, 7, 4, 8, 5, 6, 6, 6, 8, 9, 8, 5, 8, 6, 6, 5, 8, 6, 6, 6, 5, 4, 5, 5, 7, 4, 5, 7, 8, 6, 7, 5, 5, 3, 8, 7, 7, 5, 7, 4, 6, 9, 5, 6, 5, 6, 6, 6, 7, 5, 8, 8, 7, 8, 7, 7, 6, 6, 7, 5, 6, 4, 4, 4, 7, 8, 7, 5, 10, 7, 4, 7, 5, 6, 8, 6, 6, 6, 5, 7, 6, 9, 5, 6, 8, 8, 7, 5, 9, 7, 8, 7, 5, 7, 6, 7, 12, 5, 8, 6, 7, 4, 5, 6, 8, 6, 7, 5, 6, 8, 7, 4, 6, 6, 7, 7, 7, 8, 6, 7, 8, 8, 9, 4, 11, 8, 4, 9, 4, 7, 6, 7, 7, 4, 8, 6, 7, 5, 8, 4, 8, 5, 6, 9, 9, 8, 6, 7, 7, 5, 8, 4, 8, 7, 7, 5, 6, 5, 6, 8, 6, 8, 6, 7, 8, 7, 7, 7, 6, 5, 7, 5, 6, 4, 7, 4, 8, 5, 5, 8, 5, 7, 6, 7, 5, 4, 7, 7, 4, 7, 8, 12, 5, 6, 6, 7, 6, 6, 8, 9, 7, 6, 8, 8, 7, 7, 7, 4, 10, 6, 9, 7, 4, 5, 6, 6, 5, 5, 5, 5, 7, 12, 5, 4, 5, 8, 8, 4, 6, 8, 6, 8, 4, 9, 7, 8, 5, 4, 5, 4, 7, 7, 5, 7, 6, 7, 10, 6, 10, 7, 4, 5, 8, 6, 7, 5, 8, 7, 5, 6, 10, 7, 5, 4, 7, 7, 6, 7, 4, 9, 4, 7, 8, 6, 7, 6, 5, 4, 6, 7, 6, 7, 4, 6, 6, 5, 6, 5, 6, 9, 8, 7, 7, 5, 6, 6, 11, 4, 7, 8, 8, 4, 6, 5, 5, 7, 8, 8, 7, 5, 6, 8, 6, 10, 6, 8, 7, 10, 6, 6, 9, 8, 9, 4, 5, 7, 6, 6, 8, 7, 7, 5, 9, 6, 7, 7, 7, 7, 7, 7, 9, 7, 8, 9, 6, 6, 9, 6, 7, 10, 5, 6, 7, 6, 6, 4, 5, 7, 5, 7, 7, 6, 7, 5, 5, 6, 7, 4, 5, 5, 8, 7, 4, 7, 6, 4, 5, 11, 7, 7, 8, 5, 7, 6, 4, 9, 10, 8, 7, 4, 7, 9, 5, 8, 7, 9, 7, 6, 11, 4, 8, 7, 9, 8, 5, 7, 5, 5, 4, 6, 7, 6, 5, 5, 8, 8, 7, 6, 7, 7, 8, 6, 5, 5, 6, 8, 7, 6, 5, 9, 5, 6, 6, 7, 6, 4, 6, 4, 6, 6, 8, 4, 4, 6, 6, 7, 6, 7, 7, 6, 8, 8, 8, 7, 6, 7, 6, 9, 5, 4, 9, 4, 4, 6, 6, 6, 8, 7, 8, 5, 6, 6, 6, 6, 6, 7, 7, 9, 10, 7, 4, 7, 10, 7, 7, 11, 5, 6, 7, 7, 6, 9, 6, 8, 10, 12, 5, 5, 9, 7, 11, 6, 10, 6, 5, 7, 9, 6, 7, 5, 4, 5, 7, 9, 6, 5, 5, 5, 7, 6, 5, 7, 6, 7, 7, 5, 5, 5, 7, 5, 5, 5, 6, 4, 9, 5, 5, 8, 11, 6, 8, 10, 6, 7, 7, 5, 6, 6, 7, 8, 7, 8, 7, 7, 6, 4, 4, 5, 7, 4, 4, 7, 5, 7, 7, 9, 10, 4, 7, 8, 4, 6, 6, 5, 7, 9, 7, 7, 6, 5, 6, 6, 6, 4, 7, 6, 5, 6, 6, 6, 4, 8, 5, 9, 8, 5, 6, 6, 5, 8, 4, 8, 6, 5, 7, 11, 6, 5, 6, 4, 6, 6, 4, 4, 5, 5, 5, 7, 9, 7, 9, 5, 6, 6, 6, 6, 9, 5, 7, 5, 6, 5, 6, 4, 6, 5, 7, 8, 6, 5, 9, 6, 4, 7, 7, 9, 6, 3, 9, 6, 6, 8, 4, 9, 5, 5, 6, 8, 7, 7, 5, 5, 7, 9, 7, 7, 5, 5, 4, 4, 6, 5, 8, 9, 4, 6, 5, 4, 5, 4, 7, 4, 6, 6, 6, 4, 6, 4, 6, 7, 8, 5, 7, 7, 7, 7, 7, 4, 6, 6, 5, 7, 7, 7, 7, 6, 4, 6, 6, 4, 5, 7, 7, 5, 5, 6, 6, 7, 5, 6, 7, 8, 6, 5, 4, 7, 9, 7, 6, 4, 6, 6, 7, 5, 5, 8, 6, 5, 7, 4, 7, 6, 7, 7, 7, 11, 6, 7, 10, 5, 8, 7, 5, 6, 7, 8, 6, 4, 7, 7, 6, 5, 5, 4, 6, 5, 8, 5, 5, 4, 5, 7, 8, 6, 7, 7, 5, 5, 9, 5, 9, 4, 5, 6, 5, 7, 6, 6, 8, 6, 7, 7, 8, 5, 6, 7, 5, 6, 7, 4, 7, 7, 6, 4, 5, 7, 5, 4, 6, 7, 7, 5, 6, 5, 10, 11, 5, 12, 7, 8, 5, 6, 6, 8, 5, 6, 5, 10, 5, 4, 4, 7, 11, 6, 7, 7, 7, 6, 6, 5, 6, 9, 4, 6, 10, 5, 5, 6, 4, 7, 6, 11, 7, 4, 7, 6, 6, 6, 6, 7, 6, 6, 4, 6, 6, 6, 7, 4, 7, 5, 6, 5, 6, 6, 4, 4, 7, 6, 7, 6, 8, 8, 8, 6, 5, 5, 5, 6, 7, 4, 6, 4, 6, 7, 6, 4, 9, 7, 6, 7, 7, 4, 6, 7, 6, 6, 8, 6, 7, 7, 6, 6, 8, 6, 5, 6, 7, 6, 7, 9, 4, 5, 8, 5, 5, 5, 7, 8, 7, 7, 5, 10, 6, 6, 8, 7, 8, 7, 9, 4, 7, 6, 4, 5, 6, 5, 6, 9, 4, 4, 10, 9, 8, 4, 5, 9, 6, 8, 6, 8, 8, 5, 6, 5, 7, 8, 4, 7, 9, 4, 6, 6, 7, 9, 6, 6, 5, 6, 5, 6, 4, 8, 8, 8, 6, 6, 5, 5, 7, 8, 8, 5, 8, 7, 7, 5, 6, 6, 5, 4, 5, 5, 6, 6, 4, 7, 7, 6, 8, 4, 5, 6, 6, 7, 4, 8, 8, 6, 6, 4, 6, 7, 4, 7, 4, 4, 4, 7, 4, 8, 6, 7, 5, 6, 9, 7, 4, 8, 6, 4, 9, 4, 5, 7, 9, 4, 7, 5, 9, 5, 5, 8, 7, 6, 7, 6, 5, 7, 4, 6, 10, 4, 5, 4, 5, 8, 6, 8, 7, 4, 7, 9, 7, 7, 6, 6, 5, 6, 6, 4, 7, 6, 8, 6, 8, 6, 5, 4, 6, 6, 6, 9, 8, 8, 6, 7, 5, 7, 7, 5, 8, 5, 10, 6, 6, 6, 7, 6, 10, 7, 7, 5, 8, 5, 5, 8, 4, 7, 6, 4, 7, 5, 6, 7, 8, 6, 6, 8, 5, 7, 7, 6, 5, 6, 6, 5, 6, 5, 6, 5, 5, 5, 7, 7, 5, 8, 5, 10, 7, 7, 7, 7, 7, 7, 6, 7, 7, 6, 7, 10, 7, 6, 8, 8, 3, 8, 9, 5, 7, 8, 9, 8, 7, 5, 9, 7, 6, 5, 7, 10, 6, 6, 7, 8, 7, 4, 7, 6, 5, 4, 6, 7, 5, 7, 4, 8, 7, 8, 6, 9, 10, 5, 7, 8, 7, 6, 7, 4, 5, 7, 5, 5, 8, 7, 5, 8, 5, 4, 4, 7, 6, 6, 8, 7, 7, 7, 6, 5, 9, 6, 6, 4, 8, 9, 7, 5, 7, 7, 7, 8, 7, 6, 5, 4, 4, 11, 7, 6, 5, 8, 8, 9, 4, 7, 7, 4, 4, 7, 7, 5, 7, 9, 10, 8, 5, 5, 6, 6, 6, 1, 8, 4, 6, 5, 6, 9, 7, 5, 4, 6, 8, 9, 7, 6, 7, 5, 8, 6, 4, 6, 6, 7, 10, 7, 6, 6, 7, 7, 4, 9, 6, 6, 7, 5, 6, 5, 6, 7, 4, 6, 6, 8, 5, 7, 9, 8, 8, 8, 7, 5, 6, 9, 5, 7, 12, 5, 7, 8, 4, 6, 5, 7, 7, 4, 6, 9, 7, 5, 7, 7, 5, 9, 3, 5, 9, 7, 5, 5, 7, 11, 5, 5, 7, 7, 6, 6, 6, 5, 7, 6, 4, 4, 6, 6, 6, 10, 5, 6, 5, 4, 3, 6, 6, 6, 6, 6, 7, 7, 8, 6, 10, 7, 5, 7, 10, 7, 4, 7, 5, 6, 6, 6, 4, 4, 4, 6, 5, 5, 7, 6, 9, 4, 7, 6, 7, 5, 7, 5, 5, 5, 6, 6, 4, 9, 5, 9, 7, 5, 9, 6, 6, 6, 4, 8, 11, 9, 5, 8, 9, 5, 7, 4, 5, 8, 8, 7, 6, 5, 9, 5, 5, 7, 7, 6, 5, 7, 5, 6, 8, 6, 4, 6, 11, 8, 11, 5, 6, 5, 10, 7, 5, 5, 6, 4, 7, 8, 6, 5, 6, 6, 7, 4, 8, 6, 6, 7, 9, 6, 6, 5, 4, 4, 5, 7, 6, 6, 7, 7, 5, 6, 5, 4, 7, 6, 9, 7, 5, 7, 8, 7, 4, 6, 8, 8, 6, 5, 9, 5, 6, 5, 7, 7, 7, 7, 6, 4, 10, 6, 8, 6, 8, 5, 6, 9, 7, 5, 8, 6, 5, 4, 7, 7, 5, 7, 5, 7, 7, 5, 10, 4, 10, 4, 8, 5, 8, 7, 7, 4, 9, 4, 4, 6, 6, 7, 4, 5, 8, 7, 6, 6, 9, 6, 8, 7, 4, 6, 5, 5, 4, 6, 5, 10, 6, 7, 7, 6, 9, 7, 6, 10, 9, 6, 6, 8, 8, 6, 4, 6, 3, 8, 4, 5, 6, 6, 7, 6, 7, 7, 6, 7, 7, 5, 8, 6, 6, 8, 5, 4, 6, 6, 8, 6, 9, 5, 7, 7, 5, 6, 6, 4, 4, 7, 8, 7, 5, 5, 5, 7, 9, 7, 6, 10, 5, 6, 7, 7, 8, 6, 9, 7, 7, 5, 6, 8, 7, 9, 7, 7, 7, 7, 5, 5, 6, 7, 5, 7, 7, 6, 6, 6, 10, 7, 7, 7, 4, 9, 5, 4, 6, 7, 9, 5, 4, 6, 7, 5, 6, 6, 6, 7, 5, 10, 4, 4, 4, 7, 6, 7, 5, 9, 6, 6, 7, 6, 8, 7, 10, 6, 6, 6, 10, 6, 7, 9, 5, 4, 8, 10, 7, 6, 7, 4, 5, 6, 4, 10, 6, 10, 6, 6, 6, 7, 5, 7, 6, 5, 6, 6, 7, 6, 9, 7, 11, 4, 7, 6, 6, 7, 5, 5, 9, 5, 8, 7, 7, 4, 4, 10, 5, 5, 4, 4, 8, 5, 6, 8, 7, 5, 5, 6, 7, 6, 7, 8, 4, 7, 10, 7, 5, 6, 5, 8, 8, 7, 7, 5, 7, 5, 6, 7, 5, 7, 6, 4, 5, 7, 5, 9, 5, 7, 4, 6, 6, 5, 6, 5, 8, 6, 4, 5, 6, 5, 5, 9, 7, 8, 6, 9, 5, 8, 6, 6, 4, 6, 10, 8, 7, 7, 4, 8, 7, 8, 8, 7, 9, 7, 3, 5, 5, 5, 7, 6, 8, 6, 6, 6, 9, 5, 4, 8, 11, 6, 6, 5, 6, 7, 9, 8, 6, 10, 7, 6, 9, 7, 7, 6, 6, 5, 6, 10, 7, 6, 8, 7, 5, 4, 5, 6, 6, 7, 5, 8, 6, 5, 8, 4, 5, 4, 4, 5, 4, 9, 5, 7, 7, 7, 8, 9, 7, 7, 8, 7, 8, 7, 7, 7, 5, 6, 8, 8, 4, 6, 10, 9, 5, 5, 9, 7, 6, 8, 5, 6, 5, 5, 5, 5, 7, 5, 4, 6, 7, 8, 6, 5, 8, 6, 10, 6, 7, 6, 7, 8, 10, 6, 7, 6, 9, 6, 7, 5, 5, 6, 4, 9, 14, 6, 9, 7, 7, 5, 7, 5, 6, 9, 9, 8, 4, 8, 5, 5, 6, 6, 9, 6, 4, 6, 6, 6, 8, 7, 6, 6, 5, 4, 10, 6, 9, 7, 5, 7, 4, 7, 9, 5, 7, 9, 7, 7, 4, 8, 7, 6, 6, 8, 5, 6, 9, 5, 6, 4, 6, 6, 7, 6, 7, 7, 4, 4, 5, 9, 5, 7, 10, 7, 8, 8, 4, 7, 9, 8, 5, 5, 6, 5, 5, 6, 7, 6, 6, 7, 5, 4, 8, 10, 5, 4, 5, 6, 7, 6, 8, 5, 8, 10, 7, 5, 10, 5, 5, 6, 6, 7, 4, 5, 5, 6, 7, 5, 6, 6, 13, 8, 6, 6, 7, 5, 7, 8, 6, 6, 5, 6, 6, 7, 5, 7, 7, 7, 4, 6, 7, 5, 4, 9, 5, 6, 5, 6, 8, 7, 8, 6, 7, 4, 5, 7, 6, 5, 4, 7, 7, 5, 7, 7, 4, 9, 7, 6, 5, 6, 8, 5, 6, 6, 9, 8, 7, 5, 5, 5, 5, 8, 6, 6, 5, 8, 5, 8, 8, 7, 5, 7, 7, 6, 7, 4, 6, 12, 5, 6, 8, 4, 5, 8, 7, 4, 5, 6, 5, 7, 5, 6, 4, 8, 7, 9, 5, 6, 7, 6, 6, 7, 6, 4, 4, 8, 7, 7, 5, 5, 6, 6, 7, 6, 5, 7, 5, 8, 6, 9, 5, 8, 5, 8, 9, 10, 9, 6, 9, 5, 9, 7, 6, 10, 5, 6, 8, 6, 7, 5, 6, 4, 6, 6, 5, 6, 7, 4, 8, 5, 5, 5, 6, 6, 8, 6, 5, 6, 8, 9, 5, 4, 7, 6, 5, 8, 4, 5, 8, 7, 7, 6, 6, 9, 6, 5, 5, 6, 9, 8, 6, 5, 5, 7, 8, 4, 6, 7, 6, 5, 7, 9, 5, 6, 6, 9, 5, 7, 6, 7, 4, 8, 7, 6, 5, 7, 7, 6, 6, 10, 8, 5, 9, 8, 9, 8, 6, 6, 9, 4, 8, 7, 8, 12, 7, 8, 9, 5, 9, 7, 2, 9, 7, 4, 4, 11, 6, 4, 7, 8, 10, 8, 5, 6, 7, 6, 8, 6, 4, 4, 5, 6, 9, 7, 6, 5, 10, 6, 7, 5, 9, 6, 9, 6, 6, 8, 9, 6, 8, 7, 11, 4, 5, 9, 8, 9, 8, 6, 9, 7, 9, 6, 7, 8, 8, 8, 5, 8, 7, 8, 8, 7, 6, 4, 9, 6, 8, 6, 9, 5, 5, 10, 11, 7, 5, 7, 6, 4, 10, 7, 6, 5, 6, 7, 8, 4, 6, 6, 6, 7, 9, 7, 5, 4, 10, 7, 7, 9, 5, 8, 7, 6, 4, 4, 7, 7, 7, 8, 8, 7, 4, 7, 5, 7, 6, 6, 6, 9, 8, 4, 7, 6, 6, 7, 5, 7, 7, 7, 11, 7, 10, 7, 6, 4, 7, 5, 6, 6, 7, 6, 6, 9, 7, 6, 7, 8, 8, 9, 5, 7, 7, 7, 6, 5, 7, 6, 6, 6, 6, 7, 10, 6, 7, 5, 6, 6, 6, 8, 7, 9, 5, 6, 7, 4, 7, 6, 6, 11, 9, 8, 10, 3, 5, 2, 4, 9, 9, 7, 6, 4, 7, 11, 6, 6, 7, 7, 8, 6, 8, 6, 5, 3, 8, 2, 9, 9, 7, 8, 11, 5, 8, 5, 2, 6, 6, 5, 9, 2, 4, 6, 6, 7, 5, 8, 6, 10, 7, 4, 8, 6, 5, 9, 6, 10, 7, 7, 7, 8, 7, 8, 4, 5, 7, 5, 7, 6, 3, 7, 7, 9, 7, 7, 7, 12, 5, 7, 5, 4, 5, 6, 9, 9, 5, 6, 5, 8, 6, 7, 8, 8, 6, 6, 8, 6, 7, 5, 9, 6, 6, 7, 9, 6, 7, 4, 6, 7, 9, 6, 4, 6, 6, 9, 4, 9, 7, 6, 6, 6, 8, 5, 8, 6, 8, 6, 8, 5, 5, 6, 6, 8, 10, 7, 8, 7, 8, 9, 6, 6, 7, 8, 7, 8, 6, 9, 8, 7, 5, 10, 8, 6, 8, 8, 7, 6, 6, 9, 7, 3, 7, 6, 8, 8, 5, 7, 8, 3, 5, 4, 6, 4, 6, 9, 5, 5, 6, 8, 9, 5, 8, 7, 5, 7, 7, 6, 4, 7, 5, 4, 6, 6, 8, 4, 5, 8, 5, 6, 4, 4, 6, 5, 6, 4, 4, 8, 6, 11, 6, 6, 6, 7, 13, 8, 7, 6, 8, 11, 7, 5, 11, 9, 10, 4, 6, 6, 4, 9, 10, 6, 6, 6, 6, 7, 6, 5, 10, 6, 6, 8, 6, 7, 6, 11, 4, 6, 5, 6, 6, 8, 5, 4, 5, 6, 6, 6, 4, 4, 7, 5, 8, 9, 9, 6, 7, 7, 7, 6, 3, 5, 9, 6, 4, 4, 6, 4, 6, 6, 11, 7, 4, 8, 5, 7, 9, 6, 4, 6, 6, 5, 10, 4, 10, 4, 3, 6, 6, 11, 4, 6, 7, 5, 6, 7, 5, 4, 7, 5, 8, 9, 8, 7, 5, 4, 5, 7, 6, 6, 5, 5, 5, 7, 5, 8, 8, 6, 6, 6, 10, 11, 5, 4, 7, 6, 8, 8, 9, 5, 6, 3, 4, 7, 6, 4, 8, 8, 5, 5, 7, 10, 5, 6, 7, 10, 7, 6, 8, 6, 11, 5, 3, 5, 6, 4, 6, 5, 4, 4, 13, 7, 6, 9, 4, 7, 12, 6, 4, 5, 12, 11, 5, 10, 6, 5, 7, 8, 4, 5, 5, 11, 6, 6, 5, 6, 8, 13, 5, 6, 8, 10, 9, 6, 6, 10, 8, 11, 9, 5, 8, 4, 5, 11, 6, 7, 7, 5, 6, 9, 11, 6, 4, 5, 8, 7, 6, 5, 5, 6, 6, 6, 8, 9, 5, 4, 5, 6, 10, 6, 7, 7, 7, 8, 9, 7, 5, 7, 8, 4, 10, 7, 6, 6, 6, 5, 6, 6, 5, 3, 4, 6, 8, 9, 5, 8, 10, 5, 4, 4, 8, 6, 6, 6, 7, 9, 6, 6, 11, 8, 8, 5, 7, 9, 5, 8, 6, 10, 8, 7, 8, 4, 4, 8, 4, 6, 5, 7, 9, 4, 8, 9, 5, 8, 7, 8, 5, 7, 4, 6, 6, 9, 10, 6, 5, 4, 6, 6, 10, 9, 8, 9, 9, 7, 5, 9, 5, 5, 4, 10, 5, 8, 4, 7, 4, 7, 6, 7, 4, 8, 6, 6, 9, 7, 3, 8, 10, 5, 9, 6, 6, 4, 6, 4, 4, 5, 8, 8, 8, 8, 9, 5, 7, 6, 7, 7, 3, 4, 6, 6, 8, 4, 5, 7, 8, 5, 8, 5, 6, 4, 6, 9, 4, 8, 5, 5, 14, 5, 7, 5, 5, 6, 10, 7, 6, 6, 9, 11, 4, 6, 10, 5, 6, 8, 6, 6, 6, 10, 6, 11, 10, 9, 5, 9, 7, 5, 8, 7, 9, 5, 7, 7, 7, 7, 5, 10, 6, 4, 3, 9, 6, 7, 6, 5, 5, 9, 4, 6, 6, 9, 6, 8, 6, 7, 6, 3, 6, 4, 6, 8, 11, 6, 6, 5, 9, 8, 7, 6, 6, 7, 7, 5, 7, 5, 5, 6, 10, 6, 10, 7, 5, 8, 5, 7, 7, 7, 8, 5, 6, 5, 7, 6, 5, 9, 4, 7, 7, 10, 5, 6, 7, 5, 9, 9, 8, 7, 6, 4, 12, 5, 10, 8, 6, 8, 6, 5, 6, 6, 7, 6, 11, 6, 4, 6, 7, 5, 9, 6, 4, 8, 7, 6, 6, 7, 5, 9, 5, 10, 6, 6, 11, 9, 4, 8, 10, 6, 5, 9, 7, 12, 8, 6, 14, 13, 8, 8, 6, 12, 9, 11, 8, 10, 9, 9, 8, 7, 7, 9, 7, 7, 10, 5, 8, 7, 7, 8, 11, 10, 8, 8, 7, 5, 6, 14, 5, 13, 7, 10, 7, 5, 8, 7, 15, 7, 10, 9, 7, 12, 12, 16, 8, 11, 6, 6, 10, 7, 8, 7, 5, 11, 8, 10, 7, 7, 9, 15, 11, 8, 12, 8, 7, 15, 14, 7, 7, 9, 11, 12, 12, 12, 10, 8, 7, 11, 10, 6, 10, 7, 10, 10, 10, 9, 7, 5, 9, 6, 9, 9, 8, 5, 9, 6, 8, 15, 6, 12, 7, 11, 7, 8, 7, 5, 11, 10, 10, 9, 10, 8, 6, 10, 16, 5, 13, 5, 7, 10, 9, 12, 10, 9, 8, 5, 8, 9, 8, 8, 9, 10, 6, 10, 7, 6, 12, 8, 9, 13, 5, 9, 7, 8, 7, 10, 6, 7, 7, 5, 5, 11, 6, 5, 9, 6, 6, 7, 7, 11, 8, 5, 6, 6, 6, 7, 6, 8, 7, 6, 8, 8, 8, 8, 6, 6, 9, 8, 6, 5, 8, 6, 5, 5, 7, 10, 7, 7, 5, 5, 7, 7, 5, 7, 4, 8, 11, 9, 7, 7, 8, 7, 11, 8, 7, 4, 6, 6, 7, 6, 7, 6, 7, 7, 5, 4, 10, 6, 5, 10, 10, 6, 6, 5, 5, 7, 7, 7, 5, 10, 6, 5, 9, 6, 7, 8, 5, 12, 5, 9, 8, 6, 5, 11, 5, 7, 8, 8, 8, 4, 7, 6, 8, 17, 8, 7, 7, 5, 7, 6, 7, 11, 8, 9, 4, 7, 7, 6, 11, 8, 8, 6, 7, 5, 4, 6, 4, 6, 5, 10, 9, 8, 7, 5, 4, 9, 4, 4, 7, 5, 6, 4, 6, 5, 10, 8, 6, 7, 7, 7, 11, 7, 7, 4, 6, 7, 10, 6, 7, 6, 6, 8, 7, 7, 12, 4, 6, 6, 7, 5, 5, 8, 6, 13, 9, 6, 7, 8, 9, 5, 9, 6, 4, 2, 10, 8, 7, 5, 8, 6, 6, 5, 6, 7, 4, 6, 5, 5, 8, 10, 7, 6, 4, 6, 5, 7, 6, 9, 6, 8, 6, 7, 6, 6, 8, 8, 5, 7, 5, 7, 8, 8, 6, 4, 9, 9, 7, 8, 5, 4, 2, 7, 7, 8, 8, 9, 7, 10, 7, 5, 7, 7, 5, 6, 8, 5, 6, 10, 8, 8, 5, 9, 7, 10, 8, 5, 6, 7, 2, 5, 6, 9, 6, 10, 10, 7, 7, 4, 11, 7, 5, 11, 7, 9, 2, 6, 5, 4, 6, 7, 7, 8, 7, 8, 7, 9, 5, 6, 7, 7, 8, 9, 8, 5, 4, 8, 5, 5, 5, 7, 7, 8, 8, 7, 5, 6, 6, 6, 8, 6, 7, 5, 6, 10, 5, 7, 6, 10, 8, 7, 5, 6, 6, 6, 4, 5, 5, 7, 9, 5, 2, 5, 7, 7, 9, 7, 7, 8, 7, 10, 9, 2, 7, 5, 6, 7, 8, 9, 5, 4, 6, 5, 10, 7, 6, 6, 4, 7, 5, 7, 7, 10, 7, 10, 7, 4, 3, 8, 5, 7, 7, 5, 8, 2, 5, 5, 12, 7, 6, 7, 8, 6, 6, 2, 5, 8, 4, 5, 7, 7, 5, 5, 8, 9, 9, 7, 5, 9, 8, 9, 8, 7, 6, 5, 9, 6, 6, 10, 8, 7, 6, 6, 8, 8, 7, 6, 6, 7, 5, 4, 8, 8, 5, 7, 8, 7, 6, 6, 6, 6, 6, 6, 5, 6, 9, 8, 4, 7, 7, 5, 5, 7, 6, 4, 7, 7, 8, 10, 7, 7, 5, 6, 6, 7, 7, 8, 8, 4, 5, 6, 7, 8, 6, 6, 5, 8, 7, 10, 7, 10, 11, 7, 6, 2, 6, 6, 10, 8, 6, 6, 5, 5, 5, 8, 6, 5, 6, 7, 8, 7, 3, 6, 6, 7, 4, 7, 5, 4, 6, 6, 7, 9, 8, 8, 6, 4, 8, 7, 8, 10, 5, 8, 9, 9, 5, 7, 7, 8, 9, 6, 9, 6, 5, 5, 13, 9, 7, 7, 6, 10, 10, 6, 6, 5, 7, 7, 7, 7, 5, 6, 4, 10, 7, 7, 7, 8, 7, 8, 8, 7, 4, 7, 6, 5, 10, 7, 6, 7, 10, 7, 5, 10, 5, 6, 9, 5, 6, 5, 6, 7, 7, 5, 5, 6, 9, 9, 6, 6, 7, 5, 5, 4, 7, 7, 6, 7, 6, 6, 7, 7, 5, 7, 6, 9, 5, 7, 9, 6, 7, 10, 9, 9, 8, 8, 8, 7, 7, 6, 4, 6, 9, 6, 9, 10, 10, 8, 6, 8, 8, 8, 4, 7, 9, 7, 2, 6, 7, 5, 8, 8, 9, 4, 6, 6, 7, 4, 9, 5, 6, 5, 4, 5, 8, 10, 2, 9, 5, 5, 6, 4, 4, 5, 8, 8, 10, 5, 6, 10, 5, 4, 8, 11, 7, 5, 8, 2, 5, 7, 7, 6, 10, 7, 7, 9, 8, 9, 5, 11, 11, 4, 10, 7, 7, 10, 2, 7, 6, 5, 11, 5, 6, 6, 9, 12, 8, 7, 6, 7, 8, 7, 7, 4, 5, 8, 7, 5, 5, 10, 8, 6, 7, 8, 10, 7, 5, 5, 2, 7, 8, 5, 6, 7, 5, 7, 6, 11, 7, 10, 6, 7, 6, 9, 6, 10, 11, 4, 9, 9, 5, 5, 4, 9, 7, 5, 7, 8, 7, 9, 7, 9, 7, 5, 10, 6, 8, 7, 4, 6, 6, 7, 5, 8, 8, 4, 5, 11, 10, 6, 7, 2, 5, 9, 7, 12, 4, 5, 5, 4, 6, 5, 7, 6, 8, 8, 3, 7, 7, 8, 9, 8, 2, 5, 7, 6, 8, 8, 8, 7, 5, 8, 9, 7, 8, 9, 8, 5, 7, 5, 8, 7, 8, 8, 11, 9, 6, 11, 6, 8, 8, 8, 4, 6, 8, 4, 9, 5, 8, 5, 8, 8, 9, 7, 6, 6, 8, 6, 4, 9, 4, 8, 6, 8, 5, 6, 6, 6, 7, 9, 6, 7, 4, 5, 4, 6, 5, 8, 3, 5, 8, 7, 7, 11, 4, 8, 8, 5, 9, 4, 8, 8, 9, 5, 6, 8, 9, 6, 8, 6, 7, 6, 6, 8, 7, 7, 6, 8, 8, 5, 6, 4, 9, 9, 9, 8, 6, 7, 8, 5, 4, 8, 6, 8, 4, 6, 6, 5, 6, 4, 9, 6, 6, 8, 7, 8, 8, 6, 6, 5, 4, 9, 9, 4, 9, 7, 5, 8, 7, 6, 9, 6, 7, 4, 8, 5, 8, 6, 6, 6, 4, 5, 8, 7, 6, 7, 8, 7, 5, 4, 6, 5, 4, 6, 7, 9, 8, 3, 5, 8, 6, 7, 5, 8, 9, 5, 7, 9, 10, 6, 8, 9, 11, 9, 5, 3, 9, 5, 8, 5, 7, 9, 5, 7, 4, 4, 8, 7, 10, 9, 8, 5, 4, 8, 8, 8, 6, 7, 3, 9, 5, 3, 5, 6, 4, 6, 3, 5, 6, 6, 5, 6, 5, 9, 9, 8, 8, 7, 8, 4, 5, 6, 6, 7, 6, 4, 9, 5, 7, 5, 8, 6, 8, 9, 7, 2, 9, 7, 7, 9, 7, 5, 6, 7, 9, 8, 7, 6, 4, 8, 7, 8, 6, 5, 3, 7, 9, 8, 5, 9, 7, 6, 5, 7, 8, 5, 9, 8, 4, 7, 5, 8, 7, 5, 9, 8, 5, 3, 8, 5, 9, 8, 8, 5, 7, 5, 8, 8, 6, 8, 7, 3, 4, 6, 4, 7, 5, 6, 5, 5, 6, 5, 7, 8, 6, 5, 8, 4, 6, 6, 9, 9, 9, 7, 6, 8, 7, 5, 8, 7, 7, 8, 8, 6, 3, 8, 4, 5, 9, 7, 6, 8, 6, 6, 8, 9, 6, 7, 7, 2, 7, 5, 7, 4, 4, 8, 9, 5, 8, 2, 7, 6, 5, 8, 8, 5, 7, 9, 8, 5, 6, 8, 6, 8, 6, 5, 10, 4, 6, 5, 7, 5, 8, 10, 3, 7, 8, 8, 6, 8, 6, 9, 6, 7, 6, 7, 9, 10, 5, 5, 7, 7, 8, 6, 8, 7, 6, 8, 5, 6, 6, 8, 9, 8, 5, 4, 7, 8, 7, 7, 7, 9, 6, 4, 6, 4, 4, 7, 6, 6, 7, 6, 6, 9, 8, 10, 5, 6, 6, 7, 9, 5, 4, 6, 5, 6, 8, 8, 6, 5, 5, 6, 10, 11, 8, 5, 9, 5, 5, 8, 8, 8, 6, 6, 6, 6, 8, 5, 6, 7, 8, 7, 7, 6, 5, 6, 7, 4, 10, 9, 7, 8, 9, 8, 5, 8, 6, 6, 4, 7, 8, 10, 6, 8, 6, 6, 9, 8, 8, 8, 8, 9, 10, 7, 4, 5, 8, 4, 6, 9, 8, 7, 6, 6, 6, 9, 7, 8, 8, 6, 8, 8, 7, 8, 5, 7, 10, 5, 6, 8, 7, 5, 6, 7, 11, 7, 9, 9, 10, 4, 7, 4, 6, 8, 4, 8, 5, 7, 9, 7, 6, 5, 5, 6, 9, 8, 9, 8, 9, 6, 6, 4, 6, 6, 10, 7, 4, 5, 10, 7, 6, 4, 5, 6, 4, 7, 6, 7, 8, 9, 9, 9, 8, 6, 8, 3, 7, 9, 9, 4, 6, 9, 5, 4, 6, 7, 7, 6, 5, 8, 6, 5, 5, 7, 6, 7, 8, 4, 9, 8, 6, 9, 4, 6, 5, 5, 6, 9, 8, 6, 5, 10, 8, 4, 4, 7, 5, 7, 6, 6, 8, 7, 9, 5, 7, 10, 4, 5, 9, 6, 7, 4, 7, 8, 8, 6, 8, 7, 6, 6, 7, 3, 5, 9, 5, 7, 7, 6, 7, 9, 8, 5, 9, 9, 8, 4, 5, 8, 6, 10, 4, 4, 6, 6, 8, 6, 4, 7, 9, 6, 6, 6, 9, 8, 7, 9, 4, 7, 4, 8, 8, 7, 5, 12, 7, 6, 5, 8, 5, 7, 8, 6, 6, 5, 6, 8, 8, 9, 10, 9, 7, 8, 6, 4, 6, 4, 6, 8, 9, 4, 8, 4, 6, 6, 5, 4, 5, 9, 8, 6, 10, 8, 5, 6, 5, 7, 5, 8, 8, 3, 4, 6, 5, 5, 6, 5, 5, 6, 6, 5, 5, 6, 6, 7, 11, 6, 8, 8, 5, 8, 8, 7, 7, 7, 6, 3, 4, 3, 4, 4, 4, 5, 3, 4, 2, 3, 3, 4, 2, 3, 3, 3, 4, 4, 3, 2, 5, 3, 2, 5, 4, 3, 4, 4, 4, 3, 3, 3, 3, 3, 2, 4, 4, 3, 2, 3, 2, 4, 4, 4, 3, 3, 3, 3, 5, 4, 4, 4, 2, 2, 3, 2, 3, 4, 4, 2, 4, 3, 4, 3, 4, 3, 4, 4, 4, 2, 2, 5, 3, 3, 4, 3, 7, 7, 7, 6, 8, 6, 7, 6, 6, 9, 6, 5, 10, 8, 8, 8, 9, 5, 11, 9, 6, 6, 10, 7, 9, 7, 8, 9, 9, 11, 9, 9, 10, 8, 6, 5, 6, 4, 5, 8, 10, 7, 8, 9, 5, 6, 9, 10, 6, 6, 9, 3, 7, 7, 8, 4, 7, 9, 7, 5, 5, 9, 10, 5, 7, 8, 7, 8, 6, 4, 7, 6, 5, 6, 7, 5, 7, 11, 6, 8, 10, 8, 6, 9, 10, 6, 8, 5, 9, 10, 6, 5, 5, 5, 6, 11, 8, 7, 7, 7, 9, 5, 6, 10, 10, 8, 4, 5, 3, 8, 7, 9, 7, 11, 10, 10, 7, 6, 12, 9, 7, 5, 7, 7, 7, 9, 5, 6, 5, 7, 9, 7, 6, 7, 11, 7, 6, 6, 6, 2, 6, 7, 6, 8, 6, 6, 6, 4, 6, 7, 6, 8, 4, 8, 7, 7, 5, 6, 4, 8, 8, 5, 7, 6, 9, 5, 8, 4, 6, 7, 7, 5, 7, 5, 6, 9, 10, 7, 7, 7, 9, 11, 8, 6, 7, 7, 10, 5, 5, 6, 10, 10, 11, 5, 10, 7, 8, 8, 7, 6, 6, 5, 8, 8, 9, 8, 8, 11, 7, 7, 8, 11, 10, 13, 12, 7, 10, 5, 7, 8, 9, 6, 11, 7, 7, 13, 7, 10, 8, 7, 8, 7, 9, 10, 10, 7, 5, 8, 8, 8, 9, 7, 9, 9, 11, 7, 8, 9, 7, 11, 6, 7, 10, 8, 9, 8, 7, 12, 6, 8, 8, 8, 9, 10, 10, 13, 5, 11, 8, 4, 6, 10, 9, 6, 8, 6, 11, 8, 12, 10, 11, 7, 14, 8, 9, 10, 5, 12, 6, 9, 6, 9, 11, 10, 11, 7, 9, 8, 7, 8, 5, 9, 6, 11, 6, 11, 11, 9, 9, 12, 8, 12, 10, 7, 7, 7, 9, 8, 5, 7, 6, 12, 8, 10, 4, 7, 7, 5, 5, 8, 6, 9, 7, 8, 8, 7, 6, 6, 9, 7, 8, 7, 10, 9, 7, 9, 8, 7, 5, 9, 11, 8, 7, 8, 8, 3, 8, 9, 12, 9, 9, 8, 8, 9, 10, 8, 7, 13, 6, 11, 6, 7, 8, 6, 8, 6, 8, 7, 9, 10, 10, 9, 5, 6, 7, 6, 8, 10, 9, 9, 10, 6, 7, 8, 5, 6, 7, 8, 10, 11, 5, 7, 10, 11, 10, 8, 12, 5, 12, 8, 6, 7, 7, 6, 5, 8, 7, 10, 8, 10, 5, 8, 14, 8, 9, 12, 9, 6, 9, 7, 6, 6, 6, 10, 2, 7, 8, 12, 9, 4, 6, 8, 8, 10, 9, 11, 6, 9, 7, 7, 7, 10, 6, 8, 8, 8, 8, 9, 10, 7, 6, 9, 8, 9, 8, 9, 8, 7, 8, 11, 8, 8, 8, 6, 7, 8, 8, 8, 6, 6, 7, 11, 9, 9, 7, 9, 6, 6, 12, 7, 7, 9, 8, 8, 3, 8, 10, 12, 10, 8, 8, 5, 10, 9, 11, 9, 8, 8, 8, 10, 5, 8, 9, 5, 7, 6, 9, 6, 7, 7, 7, 6, 12, 7, 5, 6, 6, 6, 7, 8, 6, 7, 12, 6, 10, 7, 8, 10, 7, 9, 7, 8, 7, 9, 10, 8, 7, 5, 8, 11, 7, 8, 9, 9, 8, 12, 9, 11, 9, 10, 11, 7, 8, 7, 9, 8, 9, 8, 7, 10, 5, 5, 16, 10, 8, 8, 6, 8, 9, 6, 4, 12, 8, 10, 11, 10, 6, 6, 5, 7, 7, 8, 12, 7, 8, 7, 8, 11, 9, 6, 7, 9, 7, 8, 6, 9, 9, 7, 5, 8, 10, 7, 7, 9, 4, 9, 7, 10, 9, 7, 8, 6, 11, 8, 7, 5, 6, 11, 10, 8, 8, 11, 7, 5, 7, 10, 7, 8, 7, 9, 8, 7, 9, 4, 9, 11, 9, 7, 8, 10, 7, 8, 9, 7, 7, 5, 9, 13, 10, 9, 9, 7, 4, 7, 6, 7, 10, 11, 9, 9, 8, 4, 10, 7, 7, 9, 8, 7, 9, 8, 9, 8, 6, 7, 8, 13, 8, 10, 7, 10, 10, 11, 7, 8, 5, 11, 8, 14, 9, 9, 10, 10, 9, 6, 5, 9, 7, 8, 9, 5, 6, 7, 10, 9, 11, 7, 5, 8, 8, 11, 3, 8, 7, 7, 8, 7, 10, 6, 7, 7, 4, 11, 8, 8, 9, 6, 9, 8, 8, 14, 8, 6, 12, 11, 8, 7, 8, 10, 10, 6, 10, 5, 8, 8, 9, 5, 11, 10, 9, 9, 9, 9, 7, 5, 7, 8, 10, 7, 10, 10, 6, 11, 8, 7, 5, 9, 9, 8, 10, 10, 12, 9, 7, 4, 10, 10, 8, 9, 4, 3, 10, 10, 6, 7, 9, 10, 7, 10, 6, 11, 9, 8, 9, 9, 8, 6, 9, 10, 9, 8, 6, 9, 9, 8, 8, 11, 8, 6, 10, 8, 8, 7, 10, 7, 10, 9, 8, 8, 7, 6, 9, 4, 13, 10, 8, 8, 7, 6, 7, 9, 3, 10, 12, 10, 12, 9, 8, 6, 7, 7, 7, 9, 10, 10, 7, 10, 9, 7, 8, 10, 6, 9, 7, 9, 9, 7, 5, 7, 5, 6, 9, 8, 8, 8, 9, 8, 11, 8, 9, 8, 8, 8, 7, 7, 7, 7, 8, 11, 14, 8, 9, 9, 6, 11, 7, 8, 7, 8, 10, 8, 7, 9, 6, 6, 6, 12, 2, 8, 9, 7, 7, 10, 8, 6, 9, 7, 6, 7, 9, 8, 7, 9, 11, 5, 8, 6, 5, 10, 7, 10, 6, 9, 8, 9, 5, 9, 9, 9, 7, 7, 10, 9, 8, 9, 9, 11, 7, 8, 8, 6, 15, 9, 10, 5, 10, 10, 6, 7, 3, 9, 8, 11, 10, 6, 7, 8, 10, 9, 7, 10, 5, 8, 6, 9, 7, 10, 10, 6, 9, 9, 6, 10, 11, 6, 7, 8, 6, 6, 7, 8, 5, 7, 6, 6, 7, 8, 10, 5, 8, 9, 11, 8, 7, 11, 9, 11, 8, 8, 9, 9, 6, 6, 10, 2, 13, 8, 5, 13, 9, 8, 7, 9, 7, 7, 8, 9, 6, 12, 7, 10, 6, 6, 8, 9, 8, 9, 8, 7, 9, 9, 10, 8, 7, 8, 5, 9, 7, 8, 10, 4, 6, 11, 10, 9, 7, 7, 6, 10, 6, 7, 6, 5, 7, 4, 8, 6, 10, 8, 9, 7, 10, 7, 7, 10, 9, 8, 6, 7, 10, 8, 7, 8, 4, 7, 6, 8, 10, 13, 8, 9, 8, 8, 8, 5, 8, 7, 8, 8, 7, 9, 10, 9, 9, 6, 7, 8, 6, 5, 11, 4, 7, 7, 10, 9, 7, 8, 10, 8, 4, 5, 6, 7, 8, 10, 8, 8, 6, 6, 8, 10, 9, 10, 7, 8, 8, 10, 8, 8, 6, 8, 7, 7, 8, 9, 11, 9, 8, 9, 6, 5, 10, 11, 7, 10, 6, 6, 7, 7, 3, 5, 7, 7, 6, 5, 9, 9, 7, 5, 10, 11, 11, 8, 7, 8, 8, 7, 9, 7, 8, 10, 8, 8, 10, 4, 6, 7, 10, 8, 10, 11, 12, 6, 8, 8, 7, 11, 10, 6, 7, 6, 11, 9, 10, 10, 5, 10, 6, 9, 9, 6, 8, 8, 6, 6, 8, 7, 11, 7, 11, 7, 7, 6, 10, 7, 6, 10, 9, 9, 11, 5, 8, 11, 6, 9, 8, 9, 9, 9, 8, 7, 7, 8, 9, 10, 9, 7, 7, 7, 7, 9, 6, 5, 10, 6, 7, 7, 7, 9, 7, 10, 6, 12, 5, 5, 10, 9, 8, 6, 8, 9, 7, 10, 7, 9, 8, 4, 10, 7, 9, 8, 9, 8, 5, 10, 6, 11, 7, 10, 7, 9, 7, 9, 8, 11, 8, 8, 5, 6, 7, 6, 7, 7, 6, 8, 9, 7, 11, 9, 12, 11, 5, 6, 11, 11, 11, 9, 9, 9, 8, 9, 5, 11, 5, 11, 7, 9, 8, 8, 8, 9, 9, 10, 7, 6, 11, 5, 5, 9, 9, 13, 10, 9, 7, 8, 6, 9, 8, 5, 7, 6, 7, 6, 7, 8, 10, 5, 9, 9, 6, 9, 8, 8, 6, 8, 9, 5, 7, 6, 9, 7, 13, 7, 8, 7, 8, 10, 9, 3, 11, 8, 7, 6, 6, 6, 10, 10, 8, 9, 10, 11, 9, 7, 7, 5, 6, 6, 10, 6, 9, 6, 7, 9, 6, 6, 10, 11, 7, 7, 6, 7, 8, 7, 9, 10, 10, 8, 7, 10, 7, 8, 4, 7, 9, 10, 8, 6, 7, 8, 10, 6, 5, 10, 11, 5, 7, 10, 10, 7, 8, 10, 8, 12, 10, 9, 6, 7, 9, 4, 12, 6, 9, 7, 9, 10, 7, 10, 8, 11, 5, 7, 5, 13, 9, 7, 3, 11, 10, 8, 9, 12, 10, 10, 9, 10, 8, 3, 8, 10, 7, 5, 8, 5, 6, 7, 7, 12, 7, 8, 9, 7, 7, 10, 9, 9, 7, 11, 3, 5, 10, 10, 7, 7, 10, 5, 8, 6, 12, 7, 7, 11, 7, 9, 6, 5, 5, 7, 10, 7, 6, 4, 8, 6, 9, 8, 9, 6, 5, 11, 11, 7, 8, 9, 10, 8, 8, 7, 8, 15, 11, 6, 9, 8, 9, 7, 10, 8, 8, 7, 9, 6, 6, 8, 3, 8, 9, 10, 9, 8, 7, 10, 8, 4, 10, 5, 7, 6, 7, 11, 6, 7, 8, 8, 9, 7, 6, 9, 6, 10, 5, 6, 10, 7, 7, 12, 8, 9, 11, 13, 11, 7, 8, 12, 9, 6, 5, 8, 10, 8, 6, 7, 10, 9, 8, 7, 7, 7, 7, 9, 10, 7, 9, 4, 8, 8, 9, 6, 10, 11, 6, 8, 10, 5, 9, 6, 8, 6, 6, 10, 9, 8, 4, 7, 10, 6, 9, 12, 7, 9, 8, 7, 6, 7, 7, 6, 8, 10, 7, 6, 9, 7, 3, 8, 8, 7, 7, 10, 11, 7, 4, 6, 8, 7, 8, 4, 9, 9, 9, 12, 10, 8, 8, 8, 4, 10, 8, 7, 8, 10, 7, 9, 6, 5, 11, 8, 6, 5, 6, 7, 6, 8, 7, 6, 9, 7, 8, 5, 5, 5, 4, 7, 8, 6, 8, 7, 7, 7, 8, 6, 8, 10, 9, 9, 7, 7, 9, 7, 14, 10, 6, 7, 9, 8, 9, 6, 12, 7, 7, 9, 7, 11, 10, 10, 9, 6, 11, 8, 10, 10, 7, 7, 10, 11, 9, 9, 7, 9, 7, 6, 6, 10, 5, 9, 6, 9, 8, 7, 10, 9, 6, 8, 8, 10, 8, 7, 6, 8, 9, 3, 5, 7, 10, 7, 6, 7, 9, 7, 8, 9, 8, 10, 11, 7, 7, 11, 10, 10, 6, 5, 6, 11, 5, 10, 10, 7, 10, 10, 8, 8, 10, 7, 8, 11, 5, 9, 6, 10, 7, 11, 8, 7, 7, 7, 9, 8, 8, 8, 6, 7, 10, 5, 7, 11, 6, 10, 7, 6, 7, 10, 9, 6, 8, 8, 11, 11, 8, 8, 9, 10, 6, 6, 8, 6, 8, 7, 8, 11, 8, 10, 10, 7, 7, 7, 3, 10, 5, 12, 9, 5, 8, 8, 8, 11, 7, 6, 10, 5, 7, 10, 7, 10, 7, 7, 11, 11, 9, 6, 5, 7, 6, 7, 8, 5, 7, 9, 8, 8, 11, 6, 6, 5, 8, 8, 6, 10, 9, 5, 7, 8, 8, 7, 10, 6, 9, 9, 10, 6, 6, 7, 10, 7, 9, 11, 9, 8, 6, 5, 5, 12, 8, 9, 7, 7, 9, 9, 6, 6, 7, 8, 10, 6, 10, 5, 12, 11, 4, 6, 7, 8, 9, 6, 6, 6, 8, 7, 11, 7, 11, 9, 8, 6, 9, 8, 9, 10, 5, 10, 8, 8, 11, 9, 10, 13, 7, 9, 11, 7, 9, 12, 5, 6, 10, 5, 7, 6, 9, 10, 6, 8, 9, 7, 7, 6, 9, 7, 6, 5, 6, 9, 5, 8, 10, 8, 6, 10, 9, 8, 7, 5, 6, 7, 8, 7, 6, 8, 4, 7, 9, 10, 10, 6, 5, 7, 7, 6, 8, 6, 10, 8, 9, 9, 10, 6, 5, 10, 6, 8, 6, 9, 8, 9, 9, 9, 7, 8, 9, 8, 8, 3, 7, 7, 6, 6, 10, 9, 9, 11, 10, 10, 7, 5, 9, 7, 8, 6, 6, 9, 11, 12, 11, 8, 5, 8, 6, 4, 12, 6, 5, 13, 8, 7, 13, 3, 10, 10, 8, 10, 6, 7, 8, 4, 9, 9, 8, 9, 10, 9, 3, 7, 8, 11, 10, 10, 8, 6, 8, 10, 11, 6, 10, 9, 7, 9, 12, 8, 8, 7, 7, 7, 6, 5, 11, 13, 10, 8, 10, 8, 13, 8, 6, 10, 10, 7, 9, 9, 8, 6, 7, 7, 8, 7, 9, 12, 8, 8, 10, 8, 10, 9, 11, 6, 7, 11, 8, 8, 9, 7, 10, 5, 9, 8, 7, 6, 5, 13, 9, 6, 6, 7, 5, 12, 7, 8, 9, 5, 3, 14, 6, 7, 9, 8, 8, 2, 8, 12, 4, 7, 9, 8, 8, 6, 9, 8, 11, 6, 10, 5, 10, 6, 8, 10, 8, 5, 10, 9, 7, 9, 9, 11, 7, 2, 5, 10, 7, 10, 6, 10, 8, 8, 7, 11, 8, 5, 11, 9, 8, 10, 7, 10, 9, 7, 7, 10, 7, 6, 8, 9, 5, 11, 12, 5, 7, 13, 8, 8, 7, 6, 7, 7, 6, 8, 9, 7, 10, 10, 9, 7, 10, 7, 9, 11, 14, 8, 12, 7, 10, 9, 9, 8, 9, 9, 10, 6, 9, 8, 8, 8, 9, 7, 8, 5, 11, 9, 8, 8, 6, 5, 11, 7, 9, 7, 5, 6, 10, 13, 8, 10, 9, 6, 5, 8, 10, 9, 10, 9, 9, 9, 8, 8, 7, 6, 8, 4, 7, 6, 8, 10, 10, 10, 6, 7, 6, 4, 7, 9, 3, 9, 9, 7, 7, 8, 8, 10, 9, 7, 7, 8, 7, 9, 9, 8, 6, 9, 11, 11, 8, 10, 9, 10, 9, 6, 11, 8, 4, 9, 5, 7, 9, 10, 9, 12, 6, 10, 6, 9, 8, 9, 9, 9, 9, 10, 11, 8, 8, 10, 5, 5, 10, 6, 8, 8, 13, 8, 9, 6, 12, 8, 7, 5, 9, 7, 9, 4, 12, 8, 10, 7, 6, 12, 11, 8, 7, 10, 5, 7, 9, 10, 6, 10, 4, 8, 10, 7, 9, 9, 8, 7, 11, 9, 11, 6, 12, 7, 9, 7, 10, 8, 9, 11, 8, 11, 5, 7, 7, 7, 10, 7, 10, 8, 8, 7, 8, 7, 7, 5, 6, 8, 9, 9, 13, 8, 10, 9, 6, 8, 4, 7, 7, 14, 8, 7, 10, 9, 7, 12, 8, 7, 7, 3, 9, 7, 9, 9, 8, 8, 10, 9, 10, 8, 7, 9, 5, 6, 4, 10, 10, 11, 9, 8, 8, 9, 9, 10, 11, 9, 5, 8, 5, 10, 10, 10, 8, 10, 9, 9, 9, 7, 9, 7, 9, 8, 6, 11, 9, 7, 6, 7, 7, 11, 7, 7, 7, 8, 8, 11, 9, 7, 10, 9, 8, 12, 9, 8, 8, 10, 10, 7, 9, 9, 9, 9, 9, 10, 11, 6, 8, 4, 7, 8, 7, 10, 14, 9, 6, 6, 7, 8, 6, 9, 8, 10, 6, 8, 5, 7, 8, 9, 5, 6, 8, 5, 6, 9, 7, 10, 9, 4, 4, 4, 8, 5, 6, 8, 7, 9, 7, 8, 7, 10, 6, 4, 4, 4, 8, 7, 8, 5, 6, 8, 9, 7, 6, 5, 8, 7, 8, 5, 4, 8, 10, 7, 5, 6, 6, 8, 6, 3, 5, 6, 8, 9, 5, 8, 7, 4, 5, 6, 7, 5, 6, 5, 6, 7, 8, 6, 6, 6, 9, 11, 7, 5, 6, 6, 6, 5, 8, 6, 5, 8, 9, 6, 9, 7, 8, 6, 5, 5, 5, 6, 5, 6, 8, 4, 6, 5, 4, 10, 6, 8, 4, 5, 5, 12, 7, 5, 6, 4, 6, 4, 7, 5, 7, 7, 4, 4, 6, 7, 4, 4, 2, 6, 9, 6, 5, 5, 7, 5, 10, 6, 8, 5, 8, 4, 8, 6, 7, 9, 4, 7, 2, 5, 7, 5, 6, 6, 4, 4, 8, 8, 4, 5, 5, 7, 6, 6, 6, 4, 5, 7, 5, 4, 7, 6, 8, 4, 6, 10, 7, 6, 5, 6, 5, 11, 8, 5, 8, 5, 6, 3, 4, 6, 7, 8, 7, 6, 7, 10, 5, 6, 6, 6, 6, 8, 5, 6, 3, 7, 8, 5, 4, 5, 6, 5, 5, 7, 6, 7, 9, 11, 7, 7, 6, 8, 5, 6, 4, 5, 5, 3, 6, 8, 5, 6, 5, 7, 4, 7, 6, 7, 7, 4, 7, 6, 8, 6, 5, 4, 6, 6, 7, 5, 10, 7, 8, 7, 3, 5, 4, 5, 5, 6, 6, 9, 7, 7, 7, 10, 7, 6, 6, 7, 6, 7, 8, 6, 3, 6, 5, 6, 9, 7, 4, 6, 4, 6, 5, 6, 6, 8, 7, 7, 6, 8, 7, 8, 7, 8, 9, 4, 6, 6, 6, 6, 10, 8, 7, 8, 6, 10, 7, 5, 5, 10, 6, 7, 5, 10, 6, 7, 9, 3, 5, 7, 2, 2, 5, 8, 6, 7, 2, 4, 7, 3, 3, 3, 4, 6, 2, 3, 4, 4, 3, 5, 3, 4, 5, 5, 2, 5, 5, 2, 5, 3, 6, 5, 4, 5, 3, 3, 2, 3, 2, 3, 4, 3, 3, 3, 3, 4, 4, 4, 4, 2, 4, 4, 3, 4, 2, 2, 4, 4, 3, 4, 2, 3, 4, 5, 5, 2, 2]\n",
            "self._max_seq_length= 19\n",
            "class_counts= {'English': 2080, 'Russian': 1661, 'Arabic': 1122, 'Japanese': 542, 'Italian': 420, 'German': 403, 'Czech': 289, 'Spanish': 180, 'Dutch': 165, 'French': 160, 'Chinese': 154, 'Irish': 128, 'Greek': 109, 'Polish': 84, 'Korean': 53, 'Scottish': 52, 'Vietnamese': 40, 'Portuguese': 38}\n",
            "class_counts.items()= dict_items([('English', 2080), ('Russian', 1661), ('Arabic', 1122), ('Japanese', 542), ('Italian', 420), ('German', 403), ('Czech', 289), ('Spanish', 180), ('Dutch', 165), ('French', 160), ('Chinese', 154), ('Irish', 128), ('Greek', 109), ('Polish', 84), ('Korean', 53), ('Scottish', 52), ('Vietnamese', 40), ('Portuguese', 38)])\n",
            "sorted_counts= [('Arabic', 1122), ('Chinese', 154), ('Czech', 289), ('Dutch', 165), ('English', 2080), ('French', 160), ('German', 403), ('Greek', 109), ('Irish', 128), ('Italian', 420), ('Japanese', 542), ('Korean', 53), ('Polish', 84), ('Portuguese', 38), ('Russian', 1661), ('Scottish', 52), ('Spanish', 180), ('Vietnamese', 40)]\n",
            "frequencies= [1122, 154, 289, 165, 2080, 160, 403, 109, 128, 420, 542, 53, 84, 38, 1661, 52, 180, 40]\n",
            "contents= {'token_to_idx': {'<MASK>': 0, '<UNK>': 1, '<BEGIN>': 2, '<END>': 3, 'T': 4, 'o': 5, 't': 6, 'a': 7, 'h': 8, 'A': 9, 'b': 10, 'u': 11, 'd': 12, 'F': 13, 'k': 14, 'r': 15, 'y': 16, 'S': 17, 'e': 18, 'g': 19, 'C': 20, 'm': 21, 'H': 22, 'i': 23, 'K': 24, 'n': 25, 'W': 26, 's': 27, 'f': 28, 'G': 29, 'M': 30, 'l': 31, 'B': 32, 'z': 33, 'N': 34, 'I': 35, 'w': 36, 'D': 37, 'Q': 38, 'j': 39, 'E': 40, 'R': 41, 'Z': 42, 'c': 43, 'Y': 44, 'J': 45, 'L': 46, 'O': 47, '-': 48, 'P': 49, 'X': 50, 'p': 51, ':': 52, 'v': 53, 'U': 54, '1': 55, 'V': 56, 'x': 57, 'q': 58, 'é': 59, 'É': 60, \"'\": 61, 'ß': 62, 'ö': 63, 'ä': 64, 'ü': 65, 'ú': 66, 'à': 67, 'ò': 68, 'è': 69, 'ó': 70, 'Ś': 71, 'ą': 72, 'ń': 73, 'á': 74, 'ż': 75, 'õ': 76, 'í': 77, 'ñ': 78, 'Á': 79}, 'unk_token': '<UNK>', 'mask_token': '<MASK>', 'begin_seq_token': '<BEGIN>', 'end_seq_token': '<END>'}\n",
            "get_vectorizer()\n",
            "self._vectorizer= <__main__.SurnameVectorizer object at 0x7f2ce8731810>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RICc-V283hzn"
      },
      "source": [
        "## Training Routine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "4Bq1fIYx3hzn"
      },
      "source": [
        "def make_train_state(args):\n",
        "    return {'stop_early': False,\n",
        "            'early_stopping_step': 0,\n",
        "            'early_stopping_best_val': 1e8,\n",
        "            'learning_rate': args.learning_rate,\n",
        "            'epoch_index': 0,\n",
        "            'train_loss': [],\n",
        "            'train_acc': [],\n",
        "            'val_loss': [],\n",
        "            'val_acc': [],\n",
        "            'test_loss': -1,\n",
        "            'test_acc': -1,\n",
        "            'model_filename': args.model_state_file}\n",
        "\n",
        "\n",
        "def update_train_state(args, model, train_state):\n",
        "    \"\"\"Handle the training state updates.\n",
        "\n",
        "    Components:\n",
        "     - Early Stopping: Prevent overfitting.\n",
        "     - Model Checkpoint: Model is saved if the model is better\n",
        "    \n",
        "    :param args: main arguments\n",
        "    :param model: model to train\n",
        "    :param train_state: a dictionary representing the training state values\n",
        "    :returns:\n",
        "        a new train_state\n",
        "    \"\"\"\n",
        "\n",
        "    # Save one model at least\n",
        "    if train_state['epoch_index'] == 0:\n",
        "        torch.save(model.state_dict(), train_state['model_filename'])\n",
        "        train_state['stop_early'] = False\n",
        "\n",
        "    # Save model if performance improved\n",
        "    elif train_state['epoch_index'] >= 1:\n",
        "        loss_tm1, loss_t = train_state['val_loss'][-2:]\n",
        "         \n",
        "        # If loss worsened\n",
        "        if loss_t >= loss_tm1:\n",
        "            # Update step\n",
        "            train_state['early_stopping_step'] += 1\n",
        "        # Loss decreased\n",
        "        else:\n",
        "            # Save the best model\n",
        "            if loss_t < train_state['early_stopping_best_val']:\n",
        "                torch.save(model.state_dict(), train_state['model_filename'])\n",
        "                train_state['early_stopping_best_val'] = loss_t\n",
        "\n",
        "            # Reset early stopping step\n",
        "            train_state['early_stopping_step'] = 0\n",
        "\n",
        "        # Stop early ?\n",
        "        train_state['stop_early'] = \\\n",
        "            train_state['early_stopping_step'] >= args.early_stopping_criteria\n",
        "\n",
        "    return train_state\n",
        "\n",
        "\n",
        "def compute_accuracy(y_pred, y_target):\n",
        "    _, y_pred_indices = y_pred.max(dim=1)\n",
        "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
        "    return n_correct / len(y_pred_indices) * 100"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "c34WFqQ33hzn"
      },
      "source": [
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "    \n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                           mode='min', factor=0.5,\n",
        "                                           patience=1)\n",
        "\n",
        "train_state = make_train_state(args)\n",
        "\n",
        "epoch_bar = tqdm_notebook(desc='training routine', \n",
        "                          total=args.num_epochs,\n",
        "                          position=0)\n",
        "\n",
        "dataset.set_split('train')\n",
        "train_bar = tqdm_notebook(desc='split=train',\n",
        "                          total=dataset.get_num_batches(args.batch_size), \n",
        "                          position=1, \n",
        "                          leave=True)\n",
        "dataset.set_split('val')\n",
        "val_bar = tqdm_notebook(desc='split=val',\n",
        "                        total=dataset.get_num_batches(args.batch_size), \n",
        "                        position=1, \n",
        "                        leave=True)\n",
        "\n",
        "# try:\n",
        "for epoch_index in range(1):#args.num_epochs):\n",
        "  train_state['epoch_index'] = epoch_index\n",
        "\n",
        "  # Iterate over training dataset\n",
        "\n",
        "  # setup: batch generator, set loss and acc to 0, set train mode on\n",
        "  dataset.set_split('train')\n",
        "  batch_generator = generate_batches(dataset, \n",
        "                                      batch_size=args.batch_size, \n",
        "                                      device=args.device)\n",
        "  running_loss = 0.0\n",
        "  running_acc = 0.0\n",
        "  classifier.train()\n",
        "\n",
        "  for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    \n",
        "      # the training routine is these 5 steps:\n",
        "\n",
        "      # --------------------------------------    \n",
        "      # step 1. zero the gradients\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # step 2. compute the output\n",
        "      y_pred = classifier(x_in=batch_dict['x_data'], \n",
        "                          x_lengths=batch_dict['x_length'])\n",
        "\n",
        "      # step 3. compute the loss\n",
        "      loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "\n",
        "      running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "\n",
        "      # step 4. use loss to produce gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # step 5. use optimizer to take gradient step\n",
        "      optimizer.step()\n",
        "      # -----------------------------------------\n",
        "      # compute the accuracy\n",
        "      acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "      running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "      # update bar\n",
        "      train_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
        "      train_bar.update()\n",
        "\n",
        "  train_state['train_loss'].append(running_loss)\n",
        "  train_state['train_acc'].append(running_acc)\n",
        "\n",
        "  # Iterate over val dataset\n",
        "\n",
        "  # setup: batch generator, set loss and acc to 0; set eval mode on\n",
        "\n",
        "  dataset.set_split('val')\n",
        "  batch_generator = generate_batches(dataset, \n",
        "                                      batch_size=args.batch_size, \n",
        "                                      device=args.device)\n",
        "  running_loss = 0.\n",
        "  running_acc = 0.\n",
        "  classifier.eval()\n",
        "\n",
        "  for batch_index, batch_dict in enumerate(batch_generator):\n",
        "      # compute the output\n",
        "      y_pred = classifier(x_in=batch_dict['x_data'], \n",
        "                          x_lengths=batch_dict['x_length'])\n",
        "\n",
        "      # step 3. compute the loss\n",
        "      loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "      running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
        "\n",
        "      # compute the accuracy\n",
        "      acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "      running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "      val_bar.set_postfix(loss=running_loss, acc=running_acc, epoch=epoch_index)\n",
        "      val_bar.update()\n",
        "\n",
        "  train_state['val_loss'].append(running_loss)\n",
        "  train_state['val_acc'].append(running_acc)\n",
        "\n",
        "  train_state = update_train_state(args=args, model=classifier, \n",
        "                                    train_state=train_state)\n",
        "\n",
        "  scheduler.step(train_state['val_loss'][-1])\n",
        "\n",
        "  train_bar.n = 0\n",
        "  val_bar.n = 0\n",
        "  epoch_bar.update()\n",
        "\n",
        "  if train_state['stop_early']:\n",
        "      break\n",
        "            \n",
        "# except KeyboardInterrupt:\n",
        "#     print(\"Exiting loop\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wASoJmsx3hzq"
      },
      "source": [
        "# compute the loss & accuracy on the test set using the best available model\n",
        "\n",
        "classifier.load_state_dict(torch.load(train_state['model_filename']))\n",
        "\n",
        "classifier = classifier.to(args.device)\n",
        "dataset.class_weights = dataset.class_weights.to(args.device)\n",
        "loss_func = nn.CrossEntropyLoss(dataset.class_weights)\n",
        "\n",
        "dataset.set_split('test')\n",
        "batch_generator = generate_batches(dataset, \n",
        "                                   batch_size=args.batch_size, \n",
        "                                   device=args.device)\n",
        "running_loss = 0.\n",
        "running_acc = 0.\n",
        "classifier.eval()\n",
        "\n",
        "for batch_index, batch_dict in enumerate(batch_generator):\n",
        "    # compute the output\n",
        "    y_pred =  classifier(batch_dict['x_data'],\n",
        "                         x_lengths=batch_dict['x_length'])\n",
        "    \n",
        "    # compute the loss\n",
        "    loss = loss_func(y_pred, batch_dict['y_target'])\n",
        "    loss_t = loss.item()\n",
        "    running_loss += (loss_t - running_loss) / (batch_index + 1)\n",
        "\n",
        "    # compute the accuracy\n",
        "    acc_t = compute_accuracy(y_pred, batch_dict['y_target'])\n",
        "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
        "\n",
        "train_state['test_loss'] = running_loss\n",
        "train_state['test_acc'] = running_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1l6gM6F3hzq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6898e153-9ff8-4307-b1df-baa0ad0deab0"
      },
      "source": [
        "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
        "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 2.9559557017791684;\n",
            "Test Accuracy: 2.7108433734939745\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVBgkHDf3hzr"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goDocfrj3hzr"
      },
      "source": [
        "def predict_nationality(surname, classifier, vectorizer):\n",
        "    vectorized_surname, vec_length = vectorizer.vectorize(surname)\n",
        "    vectorized_surname = torch.tensor(vectorized_surname).unsqueeze(dim=0)\n",
        "    vec_length = torch.tensor([vec_length], dtype=torch.int64)\n",
        "    \n",
        "    result = classifier(vectorized_surname, vec_length, apply_softmax=True)\n",
        "    probability_values, indices = result.max(dim=1)\n",
        "    \n",
        "    index = indices.item()\n",
        "    prob_value = probability_values.item()\n",
        "\n",
        "    predicted_nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
        "\n",
        "    return {'nationality': predicted_nationality, 'probability': prob_value, 'surname': surname}"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sdXIH0W3hzr"
      },
      "source": [
        "# surname = input(\"Enter a surname: \")\n",
        "classifier = classifier.to(\"cpu\")\n",
        "for surname in ['McMahan', 'Nakamoto', 'Wan', 'Cho']:\n",
        "    print(predict_nationality(surname, classifier, vectorizer))\n",
        "    # {'nationality': 'Spanish', 'probability': 0.2873823642730713, 'surname': 'Cho'}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3HijYtZF3hzr"
      },
      "source": [
        ""
      ],
      "execution_count": 122,
      "outputs": []
    }
  ]
}